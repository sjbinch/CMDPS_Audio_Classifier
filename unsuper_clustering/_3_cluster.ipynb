{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_1/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_2/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_1/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_2/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/wav_정상품_V1a/#02_SX2_Normal_ES_LH_1/#02_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>./data/wav_문제품DATAa/#44_SX2_C_ES_LH_1/#44_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_ES_RH_1/#33_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_ES_RH_2/#33_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_ES_RH_1/#21_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_ES_RH_2/#21_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  fold  target category\n",
       "0    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_1/#01_...     1       0   Normal\n",
       "1    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_2/#01_...     1       0   Normal\n",
       "2    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_1/#01_...     1       0   Normal\n",
       "3    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_2/#01_...     1       0   Normal\n",
       "4    ./data/wav_정상품_V1a/#02_SX2_Normal_ES_LH_1/#02_...     1       0   Normal\n",
       "..                                                 ...   ...     ...      ...\n",
       "540  ./data/wav_문제품DATAa/#44_SX2_C_ES_LH_1/#44_SX2_...     1       3       SG\n",
       "541  ./data/wav_문제품DATAa/#33_SX2_B_ES_RH_1/#33_SX2_...     1       3       SG\n",
       "542  ./data/wav_문제품DATAa/#33_SX2_B_ES_RH_2/#33_SX2_...     1       3       SG\n",
       "543  ./data/wav_문제품DATAa/#21_SX2_A_ES_RH_1/#21_SX2_...     1       3       SG\n",
       "544  ./data/wav_문제품DATAa/#21_SX2_A_ES_RH_2/#21_SX2_...     1       3       SG\n",
       "\n",
       "[545 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "es_df = pd.read_csv('ES_meta.csv')\n",
    "es_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff39da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_2/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_1/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_2/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_1/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/wav_정상품_V1a/#02_SX2_Normal_Line_RH_2/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>./data/wav_문제품DATAa/#32_SX2_A_LINE_LH_1/#32_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_2/#33_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_1/#33_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_2/#21_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_1/#21_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  fold  target category\n",
       "0    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_2/#0...     1       0   Normal\n",
       "1    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_1/#0...     1       0   Normal\n",
       "2    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_2/#0...     1       0   Normal\n",
       "3    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_1/#0...     1       0   Normal\n",
       "4    ./data/wav_정상품_V1a/#02_SX2_Normal_Line_RH_2/#0...     1       0   Normal\n",
       "..                                                 ...   ...     ...      ...\n",
       "545  ./data/wav_문제품DATAa/#32_SX2_A_LINE_LH_1/#32_SX...     1       3       SG\n",
       "546  ./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_2/#33_SX...     1       3       SG\n",
       "547  ./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_1/#33_SX...     1       3       SG\n",
       "548  ./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_2/#21_SX...     1       3       SG\n",
       "549  ./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_1/#21_SX...     1       3       SG\n",
       "\n",
       "[550 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_df = pd.read_csv('LINE_meta.csv')\n",
    "line_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e818fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "# .wav 파일 불러오기\n",
    "\n",
    "\n",
    "# 예제 사용\n",
    "\n",
    "def get_tensor(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)  # sr=None을 설정하여 원본 샘플링 레이트를 유지\n",
    "\n",
    "    # NumPy 배열을 PyTorch 텐서로 변환\n",
    "    tensor_data = torch.tensor(audio)\n",
    "\n",
    "    # 결과 확인\n",
    "    def perform_stft(waveform, n_fft=512, win_length=None, hop_length=None, window_fn=torch.hann_window):\n",
    "        stft = torch.stft(waveform, n_fft=n_fft, win_length=win_length, hop_length=hop_length, window=window_fn(n_fft), return_complex=True)\n",
    "        return stft\n",
    "\n",
    "    stft_result = perform_stft(tensor_data)\n",
    "\n",
    "    def apply_a_weighting_librosa(stft_result, sample_rate):\n",
    "        # STFT 결과로부터 FFT 크기 추정\n",
    "        n_fft = (stft_result.size(0) - 1) * 2\n",
    "        \n",
    "        # 주파수 bins 계산\n",
    "        freqs = np.linspace(0, sample_rate / 2, stft_result.size(0))\n",
    "        \n",
    "        # A-weighting dB 값 계산\n",
    "        a_weighting_db = librosa.A_weighting(freqs)\n",
    "        \n",
    "        # dB를 power scale로 변환\n",
    "        a_weighting_scale = librosa.db_to_amplitude(a_weighting_db)\n",
    "        \n",
    "        # 텐서로 변환\n",
    "        a_weighting_scale_tensor = torch.from_numpy(a_weighting_scale).to(stft_result.dtype)\n",
    "        \n",
    "        # A-weighted STFT 계산\n",
    "        a_weighted_stft = torch.abs(stft_result).log10() + a_weighting_scale_tensor.unsqueeze(1)  # 주파수 차원에 적용\n",
    "        #a_weighted_stft = torch.abs(stft_result) * a_weighting_scale_tensor.unsqueeze(1)  # 주파수 차원에 적용\n",
    "\n",
    "        return a_weighted_stft\n",
    "\n",
    "    sample_rate = 12800  # 샘플 레이트 예제 값\n",
    "    # A-weighting 적용\n",
    "    a_weighted_result = apply_a_weighting_librosa(stft_result, sample_rate)\n",
    "    _3k_result = a_weighted_result[32:129,:] # 하나당 25이므로, 800 하려면\n",
    "    return torch.abs(_3k_result) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010e514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/librosa/core/convert.py:1870: RuntimeWarning: divide by zero encountered in log10\n",
      "  + 2 * np.log10(f_sq)\n"
     ]
    }
   ],
   "source": [
    "es_grr_list = []\n",
    "es_sg_list = []\n",
    "es_dol_list = []\n",
    "\n",
    "\n",
    "for idx in es_df.index:\n",
    "    row = es_df.loc[idx]\n",
    "    filename = row['filename']\n",
    "    if row['category'] == 'GRR':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            es_grr_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "\n",
    "    elif row['category'] == 'SG':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            es_sg_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "\n",
    "    elif row['category'] == 'DOL':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            es_dol_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac98faf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97, 1000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_grr_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0d781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/librosa/core/convert.py:1870: RuntimeWarning: divide by zero encountered in log10\n",
      "  + 2 * np.log10(f_sq)\n"
     ]
    }
   ],
   "source": [
    "rps_df = pd.read_csv('1.0RPS_meta.csv')\n",
    "\n",
    "rps_grr_list = []\n",
    "rps_sg_list = []\n",
    "rps_dol_list = []\n",
    "\n",
    "for idx in rps_df.index:\n",
    "    row = rps_df.loc[idx]\n",
    "    filename= row['filename']\n",
    "    if row['category'] == 'GRR':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            rps_grr_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "\n",
    "    elif row['category'] == 'SG':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            rps_sg_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "\n",
    "    elif row['category'] == 'DOL':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            rps_dol_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95763883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 97, 1000])\n",
      "Output Tensor Shape: torch.Size([1, 1, 97, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__() # (97, 1000) \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential( \n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 49, 500)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 25, 250)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 64, 13, 125)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 7, 63)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 4, 32)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 1, 2, 16)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output shape: (b, 16, 4, 32)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2, padding=(1, 1), output_padding=(0, 0)),  # Output shape: (b, 32, 7, 63) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 64, kernel_size=3, stride=2, padding=(1,1), output_padding=(0 ,0)),  # Output shape: (b, 64, 13, 125) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=(1,1), output_padding=(0, 1)),  # Output shape: (b, 32, 25, 250)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 16, 49, 500)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 1, 97, 1000) \n",
    "        )\n",
    "# output padding must be smaller than stride\n",
    "    def forward(self, x):\n",
    "        mid = self.encoder(x)\n",
    "        x = self.decoder(mid)\n",
    "        return x\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "test_autoencoder = Autoencoder()\n",
    "\n",
    "input_tensor = torch.randn(1, 1, 97, 1000)  # (batch_size, channels, height, width)\n",
    "\n",
    "# Forward pass through the autoencoder\n",
    "output_tensor = test_autoencoder(input_tensor)\n",
    "\n",
    "# Print the shapes of input and output tensors\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Output Tensor Shape:\", output_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "689776c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_list):\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    customdataset = CustomDataset(dataset_list)\n",
    "    train_loader = DataLoader(customdataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    autoencoder = Autoencoder().to(dev)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader): # i-th batch\n",
    "            inputs = data\n",
    "            inputs = inputs.to(dev)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'epoch : {epoch}, loss: {running_loss}')\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    encoded_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in dataset_list:\n",
    "            encoded_list.append(autoencoder.encoder(i.to(dev)).flatten())\n",
    "\n",
    "    return encoded_list\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee369a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss: 1.6104599833488464\n",
      "epoch : 1, loss: 1.6633005738258362\n",
      "epoch : 2, loss: 1.4890207648277283\n",
      "epoch : 3, loss: 1.6116323471069336\n",
      "epoch : 4, loss: 1.612727165222168\n",
      "epoch : 5, loss: 1.5507891178131104\n",
      "epoch : 6, loss: 1.4642230868339539\n",
      "epoch : 7, loss: 1.531641960144043\n",
      "epoch : 8, loss: 1.5158032774925232\n",
      "epoch : 9, loss: 1.4795319437980652\n",
      "epoch : 10, loss: 1.3708306550979614\n",
      "epoch : 11, loss: 1.3866477012634277\n",
      "epoch : 12, loss: 1.3150774240493774\n",
      "epoch : 13, loss: 1.235659122467041\n",
      "epoch : 14, loss: 1.2373822927474976\n",
      "epoch : 15, loss: 1.0702800154685974\n",
      "epoch : 16, loss: 1.0040754973888397\n",
      "epoch : 17, loss: 0.9827935695648193\n",
      "epoch : 18, loss: 0.9575589001178741\n",
      "epoch : 19, loss: 0.9589625895023346\n",
      "epoch : 20, loss: 0.8616147637367249\n",
      "epoch : 21, loss: 0.8274655342102051\n",
      "epoch : 22, loss: 0.7354930937290192\n",
      "epoch : 23, loss: 0.7025419473648071\n",
      "epoch : 24, loss: 0.6476605832576752\n",
      "epoch : 25, loss: 0.6100171506404877\n",
      "epoch : 26, loss: 0.552794486284256\n",
      "epoch : 27, loss: 0.5065419971942902\n",
      "epoch : 28, loss: 0.4201638847589493\n",
      "epoch : 29, loss: 0.384369358420372\n",
      "epoch : 30, loss: 0.34446413815021515\n",
      "epoch : 31, loss: 0.33254870772361755\n",
      "epoch : 32, loss: 0.30999623239040375\n",
      "epoch : 33, loss: 0.278134748339653\n",
      "epoch : 34, loss: 0.26461128890514374\n",
      "epoch : 35, loss: 0.26726847887039185\n",
      "epoch : 36, loss: 0.2522406280040741\n",
      "epoch : 37, loss: 0.24543070793151855\n",
      "epoch : 38, loss: 0.23949472606182098\n",
      "epoch : 39, loss: 0.23252946138381958\n",
      "epoch : 40, loss: 0.23337258398532867\n",
      "epoch : 41, loss: 0.2285718470811844\n",
      "epoch : 42, loss: 0.22349504381418228\n",
      "epoch : 43, loss: 0.2225109189748764\n",
      "epoch : 44, loss: 0.21979691088199615\n",
      "epoch : 45, loss: 0.21817465126514435\n",
      "epoch : 46, loss: 0.21776040643453598\n",
      "epoch : 47, loss: 0.21594282239675522\n",
      "epoch : 48, loss: 0.21132713556289673\n",
      "epoch : 49, loss: 0.21424465626478195\n",
      "epoch : 50, loss: 0.2132694348692894\n",
      "epoch : 51, loss: 0.21146324276924133\n",
      "epoch : 52, loss: 0.20828603208065033\n",
      "epoch : 53, loss: 0.20653381198644638\n",
      "epoch : 54, loss: 0.21006964892148972\n",
      "epoch : 55, loss: 0.21005983650684357\n",
      "epoch : 56, loss: 0.2080898880958557\n",
      "epoch : 57, loss: 0.20486032962799072\n",
      "epoch : 58, loss: 0.2071612924337387\n",
      "epoch : 59, loss: 0.21212463080883026\n",
      "epoch : 60, loss: 0.20578374713659286\n",
      "epoch : 61, loss: 0.205817811191082\n",
      "epoch : 62, loss: 0.20317873358726501\n",
      "epoch : 63, loss: 0.20882652699947357\n",
      "epoch : 64, loss: 0.20742888003587723\n",
      "epoch : 65, loss: 0.20356983691453934\n",
      "epoch : 66, loss: 0.20526007562875748\n",
      "epoch : 67, loss: 0.20386195927858353\n",
      "epoch : 68, loss: 0.2040269821882248\n",
      "epoch : 69, loss: 0.20130351930856705\n",
      "epoch : 70, loss: 0.20284078270196915\n",
      "epoch : 71, loss: 0.20172453671693802\n",
      "epoch : 72, loss: 0.2015901282429695\n",
      "epoch : 73, loss: 0.2006685808300972\n",
      "epoch : 74, loss: 0.20499443262815475\n",
      "epoch : 75, loss: 0.2041267827153206\n",
      "epoch : 76, loss: 0.20609401166439056\n",
      "epoch : 77, loss: 0.2049967348575592\n",
      "epoch : 78, loss: 0.20327863842248917\n",
      "epoch : 79, loss: 0.20207709819078445\n",
      "epoch : 80, loss: 0.20058701187372208\n",
      "epoch : 81, loss: 0.19887853413820267\n",
      "epoch : 82, loss: 0.19837044179439545\n",
      "epoch : 83, loss: 0.20141199976205826\n",
      "epoch : 84, loss: 0.20091437548398972\n",
      "epoch : 85, loss: 0.1993180215358734\n",
      "epoch : 86, loss: 0.19908877462148666\n",
      "epoch : 87, loss: 0.1984993740916252\n",
      "epoch : 88, loss: 0.20280326157808304\n",
      "epoch : 89, loss: 0.19917551428079605\n",
      "epoch : 90, loss: 0.20072899758815765\n",
      "epoch : 91, loss: 0.20371223986148834\n",
      "epoch : 92, loss: 0.1988465115427971\n",
      "epoch : 93, loss: 0.19691699743270874\n",
      "epoch : 94, loss: 0.19818837940692902\n",
      "epoch : 95, loss: 0.19962500035762787\n",
      "epoch : 96, loss: 0.1978703960776329\n",
      "epoch : 97, loss: 0.19617893546819687\n",
      "epoch : 98, loss: 0.20058197528123856\n",
      "epoch : 99, loss: 0.19565020501613617\n",
      "Finished Training\n",
      "epoch : 0, loss: 4.052967548370361\n",
      "epoch : 1, loss: 3.9513471722602844\n",
      "epoch : 2, loss: 3.7652209997177124\n",
      "epoch : 3, loss: 3.5809584856033325\n",
      "epoch : 4, loss: 2.9349245429039\n",
      "epoch : 5, loss: 2.05035263299942\n",
      "epoch : 6, loss: 1.600229263305664\n",
      "epoch : 7, loss: 1.3475377261638641\n",
      "epoch : 8, loss: 1.0311419069766998\n",
      "epoch : 9, loss: 0.840892031788826\n",
      "epoch : 10, loss: 0.7472852766513824\n",
      "epoch : 11, loss: 0.6467880308628082\n",
      "epoch : 12, loss: 0.559089332818985\n",
      "epoch : 13, loss: 0.4998762533068657\n",
      "epoch : 14, loss: 0.45293063670396805\n",
      "epoch : 15, loss: 0.42036111652851105\n",
      "epoch : 16, loss: 0.40504002571105957\n",
      "epoch : 17, loss: 0.40139249712228775\n",
      "epoch : 18, loss: 0.3992583230137825\n",
      "epoch : 19, loss: 0.3897954747080803\n",
      "epoch : 20, loss: 0.3850063681602478\n",
      "epoch : 21, loss: 0.3807564377784729\n",
      "epoch : 22, loss: 0.3786478415131569\n",
      "epoch : 23, loss: 0.3785964772105217\n",
      "epoch : 24, loss: 0.37464356422424316\n",
      "epoch : 25, loss: 0.37183280289173126\n",
      "epoch : 26, loss: 0.37002452462911606\n",
      "epoch : 27, loss: 0.371534138917923\n",
      "epoch : 28, loss: 0.3679090514779091\n",
      "epoch : 29, loss: 0.3701975643634796\n",
      "epoch : 30, loss: 0.3687010779976845\n",
      "epoch : 31, loss: 0.367235891520977\n",
      "epoch : 32, loss: 0.36594343930482864\n",
      "epoch : 33, loss: 0.3660662844777107\n",
      "epoch : 34, loss: 0.36639049649238586\n",
      "epoch : 35, loss: 0.36064980924129486\n",
      "epoch : 36, loss: 0.36425091326236725\n",
      "epoch : 37, loss: 0.3629395365715027\n",
      "epoch : 38, loss: 0.3635326400399208\n",
      "epoch : 39, loss: 0.3641880452632904\n",
      "epoch : 40, loss: 0.36101169139146805\n",
      "epoch : 41, loss: 0.3623155727982521\n",
      "epoch : 42, loss: 0.3599151521921158\n",
      "epoch : 43, loss: 0.3617512434720993\n",
      "epoch : 44, loss: 0.35778503119945526\n",
      "epoch : 45, loss: 0.3573771044611931\n",
      "epoch : 46, loss: 0.3608125224709511\n",
      "epoch : 47, loss: 0.35744280368089676\n",
      "epoch : 48, loss: 0.3581208735704422\n",
      "epoch : 49, loss: 0.35733789950609207\n",
      "epoch : 50, loss: 0.35511042922735214\n",
      "epoch : 51, loss: 0.3557668477296829\n",
      "epoch : 52, loss: 0.35556599497795105\n",
      "epoch : 53, loss: 0.3576742634177208\n",
      "epoch : 54, loss: 0.3586510345339775\n",
      "epoch : 55, loss: 0.35791293531656265\n",
      "epoch : 56, loss: 0.3529973179101944\n",
      "epoch : 57, loss: 0.35278116911649704\n",
      "epoch : 58, loss: 0.3554384931921959\n",
      "epoch : 59, loss: 0.3541749492287636\n",
      "epoch : 60, loss: 0.3507549539208412\n",
      "epoch : 61, loss: 0.3508075550198555\n",
      "epoch : 62, loss: 0.35285550355911255\n",
      "epoch : 63, loss: 0.3530668169260025\n",
      "epoch : 64, loss: 0.3508142977952957\n",
      "epoch : 65, loss: 0.3525255545973778\n",
      "epoch : 66, loss: 0.3510545641183853\n",
      "epoch : 67, loss: 0.35103772580623627\n",
      "epoch : 68, loss: 0.350646935403347\n",
      "epoch : 69, loss: 0.34937357902526855\n",
      "epoch : 70, loss: 0.34883420169353485\n",
      "epoch : 71, loss: 0.34949028491973877\n",
      "epoch : 72, loss: 0.34789392352104187\n",
      "epoch : 73, loss: 0.34870003908872604\n",
      "epoch : 74, loss: 0.3497810512781143\n",
      "epoch : 75, loss: 0.3478305786848068\n",
      "epoch : 76, loss: 0.34692322462797165\n",
      "epoch : 77, loss: 0.34989385306835175\n",
      "epoch : 78, loss: 0.3485483452677727\n",
      "epoch : 79, loss: 0.34742189198732376\n",
      "epoch : 80, loss: 0.3461510017514229\n",
      "epoch : 81, loss: 0.344732828438282\n",
      "epoch : 82, loss: 0.3450377285480499\n",
      "epoch : 83, loss: 0.3452444151043892\n",
      "epoch : 84, loss: 0.3465290665626526\n",
      "epoch : 85, loss: 0.34793398529291153\n",
      "epoch : 86, loss: 0.3466073349118233\n",
      "epoch : 87, loss: 0.3433207720518112\n",
      "epoch : 88, loss: 0.3419065624475479\n",
      "epoch : 89, loss: 0.34473029524087906\n",
      "epoch : 90, loss: 0.3431919366121292\n",
      "epoch : 91, loss: 0.34295081347227097\n",
      "epoch : 92, loss: 0.3440241292119026\n",
      "epoch : 93, loss: 0.34283868968486786\n",
      "epoch : 94, loss: 0.3423212617635727\n",
      "epoch : 95, loss: 0.34219198673963547\n",
      "epoch : 96, loss: 0.3423929661512375\n",
      "epoch : 97, loss: 0.3413429483771324\n",
      "epoch : 98, loss: 0.338847279548645\n",
      "epoch : 99, loss: 0.33866897225379944\n",
      "Finished Training\n",
      "epoch : 0, loss: 2.693657398223877\n",
      "epoch : 1, loss: 2.80112224817276\n",
      "epoch : 2, loss: 2.4111225605010986\n",
      "epoch : 3, loss: 2.609428286552429\n",
      "epoch : 4, loss: 2.2837603092193604\n",
      "epoch : 5, loss: 1.6570485830307007\n",
      "epoch : 6, loss: 1.2993602752685547\n",
      "epoch : 7, loss: 1.0390513837337494\n",
      "epoch : 8, loss: 0.8626865595579147\n",
      "epoch : 9, loss: 0.7653824836015701\n",
      "epoch : 10, loss: 0.6628171056509018\n",
      "epoch : 11, loss: 0.5719192326068878\n",
      "epoch : 12, loss: 0.5233290642499924\n",
      "epoch : 13, loss: 0.4878666326403618\n",
      "epoch : 14, loss: 0.4488431513309479\n",
      "epoch : 15, loss: 0.42214682698249817\n",
      "epoch : 16, loss: 0.41152423620224\n",
      "epoch : 17, loss: 0.4163545295596123\n",
      "epoch : 18, loss: 0.40656302869319916\n",
      "epoch : 19, loss: 0.3973392844200134\n",
      "epoch : 20, loss: 0.3997391387820244\n",
      "epoch : 21, loss: 0.38271065056324005\n",
      "epoch : 22, loss: 0.3944550231099129\n",
      "epoch : 23, loss: 0.39668115973472595\n",
      "epoch : 24, loss: 0.38590822368860245\n",
      "epoch : 25, loss: 0.39104196429252625\n",
      "epoch : 26, loss: 0.38521917164325714\n",
      "epoch : 27, loss: 0.3834622725844383\n",
      "epoch : 28, loss: 0.38726218044757843\n",
      "epoch : 29, loss: 0.38329852372407913\n",
      "epoch : 30, loss: 0.375064954161644\n",
      "epoch : 31, loss: 0.3832888603210449\n",
      "epoch : 32, loss: 0.3788391724228859\n",
      "epoch : 33, loss: 0.3765201196074486\n",
      "epoch : 34, loss: 0.38000252842903137\n",
      "epoch : 35, loss: 0.3756242096424103\n",
      "epoch : 36, loss: 0.379822239279747\n",
      "epoch : 37, loss: 0.3773788884282112\n",
      "epoch : 38, loss: 0.3786487355828285\n",
      "epoch : 39, loss: 0.3757951855659485\n",
      "epoch : 40, loss: 0.37593992054462433\n",
      "epoch : 41, loss: 0.36201976239681244\n",
      "epoch : 42, loss: 0.37440675497055054\n",
      "epoch : 43, loss: 0.37116144597530365\n",
      "epoch : 44, loss: 0.3707885071635246\n",
      "epoch : 45, loss: 0.36401960253715515\n",
      "epoch : 46, loss: 0.36921076476573944\n",
      "epoch : 47, loss: 0.37375009804964066\n",
      "epoch : 48, loss: 0.3728789910674095\n",
      "epoch : 49, loss: 0.38832706958055496\n",
      "epoch : 50, loss: 0.3760301321744919\n",
      "epoch : 51, loss: 0.36717358231544495\n",
      "epoch : 52, loss: 0.376905120909214\n",
      "epoch : 53, loss: 0.3736758902668953\n",
      "epoch : 54, loss: 0.3684435561299324\n",
      "epoch : 55, loss: 0.3659737706184387\n",
      "epoch : 56, loss: 0.3714456185698509\n",
      "epoch : 57, loss: 0.3654751032590866\n",
      "epoch : 58, loss: 0.3717108592391014\n",
      "epoch : 59, loss: 0.3668561726808548\n",
      "epoch : 60, loss: 0.36904895305633545\n",
      "epoch : 61, loss: 0.36191679537296295\n",
      "epoch : 62, loss: 0.3743039220571518\n",
      "epoch : 63, loss: 0.3680035173892975\n",
      "epoch : 64, loss: 0.36847715824842453\n",
      "epoch : 65, loss: 0.36983901262283325\n",
      "epoch : 66, loss: 0.3745853751897812\n",
      "epoch : 67, loss: 0.36900967359542847\n",
      "epoch : 68, loss: 0.3686581552028656\n",
      "epoch : 69, loss: 0.3690285012125969\n",
      "epoch : 70, loss: 0.36680512875318527\n",
      "epoch : 71, loss: 0.3689497634768486\n",
      "epoch : 72, loss: 0.3694880083203316\n",
      "epoch : 73, loss: 0.3605670630931854\n",
      "epoch : 74, loss: 0.3694924935698509\n",
      "epoch : 75, loss: 0.3670170083642006\n",
      "epoch : 76, loss: 0.36954453587532043\n",
      "epoch : 77, loss: 0.37069030851125717\n",
      "epoch : 78, loss: 0.36518799513578415\n",
      "epoch : 79, loss: 0.3593118265271187\n",
      "epoch : 80, loss: 0.3554403781890869\n",
      "epoch : 81, loss: 0.36514732986688614\n",
      "epoch : 82, loss: 0.3675883188843727\n",
      "epoch : 83, loss: 0.36821233481168747\n",
      "epoch : 84, loss: 0.3733435347676277\n",
      "epoch : 85, loss: 0.3712985888123512\n",
      "epoch : 86, loss: 0.3662959635257721\n",
      "epoch : 87, loss: 0.3655700236558914\n",
      "epoch : 88, loss: 0.36919647455215454\n",
      "epoch : 89, loss: 0.36561132222414017\n",
      "epoch : 90, loss: 0.3611891195178032\n",
      "epoch : 91, loss: 0.3714638650417328\n",
      "epoch : 92, loss: 0.36773448437452316\n",
      "epoch : 93, loss: 0.3683162033557892\n",
      "epoch : 94, loss: 0.3644101694226265\n",
      "epoch : 95, loss: 0.36423806846141815\n",
      "epoch : 96, loss: 0.36491622775793076\n",
      "epoch : 97, loss: 0.36167655140161514\n",
      "epoch : 98, loss: 0.36557674407958984\n",
      "epoch : 99, loss: 0.3634418398141861\n",
      "Finished Training\n",
      "epoch : 0, loss: 0.8125506639480591\n",
      "epoch : 1, loss: 0.8054991960525513\n",
      "epoch : 2, loss: 0.7985962629318237\n",
      "epoch : 3, loss: 0.7917321920394897\n",
      "epoch : 4, loss: 0.7848325371742249\n",
      "epoch : 5, loss: 0.7778525352478027\n",
      "epoch : 6, loss: 0.7707504034042358\n",
      "epoch : 7, loss: 0.7635074257850647\n",
      "epoch : 8, loss: 0.7561028003692627\n",
      "epoch : 9, loss: 0.7486444711685181\n",
      "epoch : 10, loss: 0.7414374947547913\n",
      "epoch : 11, loss: 0.7344396114349365\n",
      "epoch : 12, loss: 0.7270947694778442\n",
      "epoch : 13, loss: 0.7197489142417908\n",
      "epoch : 14, loss: 0.7123561501502991\n",
      "epoch : 15, loss: 0.7046598792076111\n",
      "epoch : 16, loss: 0.6966851949691772\n",
      "epoch : 17, loss: 0.6882632970809937\n",
      "epoch : 18, loss: 0.6788803339004517\n",
      "epoch : 19, loss: 0.6682738661766052\n",
      "epoch : 20, loss: 0.6556148529052734\n",
      "epoch : 21, loss: 0.6393431425094604\n",
      "epoch : 22, loss: 0.6173712015151978\n",
      "epoch : 23, loss: 0.5863630771636963\n",
      "epoch : 24, loss: 0.5419570207595825\n",
      "epoch : 25, loss: 0.4820152223110199\n",
      "epoch : 26, loss: 0.41811007261276245\n",
      "epoch : 27, loss: 0.4160124361515045\n",
      "epoch : 28, loss: 0.4074351191520691\n",
      "epoch : 29, loss: 0.3518916070461273\n",
      "epoch : 30, loss: 0.31935879588127136\n",
      "epoch : 31, loss: 0.30884432792663574\n",
      "epoch : 32, loss: 0.298989474773407\n",
      "epoch : 33, loss: 0.28134799003601074\n",
      "epoch : 34, loss: 0.2568872570991516\n",
      "epoch : 35, loss: 0.23328565061092377\n",
      "epoch : 36, loss: 0.22190776467323303\n",
      "epoch : 37, loss: 0.2204056829214096\n",
      "epoch : 38, loss: 0.20578162372112274\n",
      "epoch : 39, loss: 0.18346820771694183\n",
      "epoch : 40, loss: 0.17070142924785614\n",
      "epoch : 41, loss: 0.1658470779657364\n",
      "epoch : 42, loss: 0.16070425510406494\n",
      "epoch : 43, loss: 0.15279358625411987\n",
      "epoch : 44, loss: 0.14536409080028534\n",
      "epoch : 45, loss: 0.14268484711647034\n",
      "epoch : 46, loss: 0.14293885231018066\n",
      "epoch : 47, loss: 0.1392587274312973\n",
      "epoch : 48, loss: 0.13259242475032806\n",
      "epoch : 49, loss: 0.12850400805473328\n",
      "epoch : 50, loss: 0.12706227600574493\n",
      "epoch : 51, loss: 0.12514245510101318\n",
      "epoch : 52, loss: 0.12161813676357269\n",
      "epoch : 53, loss: 0.11809088289737701\n",
      "epoch : 54, loss: 0.11665426939725876\n",
      "epoch : 55, loss: 0.11667163670063019\n",
      "epoch : 56, loss: 0.11566527187824249\n",
      "epoch : 57, loss: 0.11388971656560898\n",
      "epoch : 58, loss: 0.11320902407169342\n",
      "epoch : 59, loss: 0.11344322562217712\n",
      "epoch : 60, loss: 0.1132158488035202\n",
      "epoch : 61, loss: 0.1120576411485672\n",
      "epoch : 62, loss: 0.11080796271562576\n",
      "epoch : 63, loss: 0.11031074076890945\n",
      "epoch : 64, loss: 0.11013579368591309\n",
      "epoch : 65, loss: 0.10942599922418594\n",
      "epoch : 66, loss: 0.10848597437143326\n",
      "epoch : 67, loss: 0.10803037881851196\n",
      "epoch : 68, loss: 0.10789459943771362\n",
      "epoch : 69, loss: 0.10751684755086899\n",
      "epoch : 70, loss: 0.10688097029924393\n",
      "epoch : 71, loss: 0.10646550357341766\n",
      "epoch : 72, loss: 0.10636941343545914\n",
      "epoch : 73, loss: 0.10615704208612442\n",
      "epoch : 74, loss: 0.1057107225060463\n",
      "epoch : 75, loss: 0.10538163781166077\n",
      "epoch : 76, loss: 0.10525108873844147\n",
      "epoch : 77, loss: 0.10503187775611877\n",
      "epoch : 78, loss: 0.10466335713863373\n",
      "epoch : 79, loss: 0.10435419529676437\n",
      "epoch : 80, loss: 0.10417342185974121\n",
      "epoch : 81, loss: 0.10394243150949478\n",
      "epoch : 82, loss: 0.10361047089099884\n",
      "epoch : 83, loss: 0.10333562642335892\n",
      "epoch : 84, loss: 0.10316021740436554\n",
      "epoch : 85, loss: 0.10293853282928467\n",
      "epoch : 86, loss: 0.1026320829987526\n",
      "epoch : 87, loss: 0.10238931328058243\n",
      "epoch : 88, loss: 0.10220257937908173\n",
      "epoch : 89, loss: 0.10196202248334885\n",
      "epoch : 90, loss: 0.10176107287406921\n",
      "epoch : 91, loss: 0.10160980373620987\n",
      "epoch : 92, loss: 0.10139279812574387\n",
      "epoch : 93, loss: 0.10123651474714279\n",
      "epoch : 94, loss: 0.10109086334705353\n",
      "epoch : 95, loss: 0.10092893987894058\n",
      "epoch : 96, loss: 0.10083317011594772\n",
      "epoch : 97, loss: 0.10069160908460617\n",
      "epoch : 98, loss: 0.10062122344970703\n",
      "epoch : 99, loss: 0.10051176697015762\n",
      "Finished Training\n",
      "epoch : 0, loss: 0.28347915410995483\n",
      "epoch : 1, loss: 0.3163503259420395\n",
      "epoch : 2, loss: 0.27610039710998535\n",
      "epoch : 3, loss: 0.2550683915615082\n",
      "epoch : 4, loss: 0.25224383920431137\n",
      "epoch : 5, loss: 0.2569502592086792\n",
      "epoch : 6, loss: 0.2617543488740921\n",
      "epoch : 7, loss: 0.23711032420396805\n",
      "epoch : 8, loss: 0.23928704857826233\n",
      "epoch : 9, loss: 0.2372814640402794\n",
      "epoch : 10, loss: 0.2489999160170555\n",
      "epoch : 11, loss: 0.26768751442432404\n",
      "epoch : 12, loss: 0.23629043996334076\n",
      "epoch : 13, loss: 0.24424317479133606\n",
      "epoch : 14, loss: 0.26818836480379105\n",
      "epoch : 15, loss: 0.24259053170681\n",
      "epoch : 16, loss: 0.23842547088861465\n",
      "epoch : 17, loss: 0.2364662066102028\n",
      "epoch : 18, loss: 0.226100854575634\n",
      "epoch : 19, loss: 0.22067580372095108\n",
      "epoch : 20, loss: 0.2167547047138214\n",
      "epoch : 21, loss: 0.20953988283872604\n",
      "epoch : 22, loss: 0.2058824598789215\n",
      "epoch : 23, loss: 0.19702140241861343\n",
      "epoch : 24, loss: 0.2010534480214119\n",
      "epoch : 25, loss: 0.195473313331604\n",
      "epoch : 26, loss: 0.19563065469264984\n",
      "epoch : 27, loss: 0.19764761626720428\n",
      "epoch : 28, loss: 0.18510375916957855\n",
      "epoch : 29, loss: 0.18535594642162323\n",
      "epoch : 30, loss: 0.1872229278087616\n",
      "epoch : 31, loss: 0.19013939797878265\n",
      "epoch : 32, loss: 0.17784150689840317\n",
      "epoch : 33, loss: 0.18316387385129929\n",
      "epoch : 34, loss: 0.1860545203089714\n",
      "epoch : 35, loss: 0.18174850940704346\n",
      "epoch : 36, loss: 0.17927635461091995\n",
      "epoch : 37, loss: 0.1835874617099762\n",
      "epoch : 38, loss: 0.17940902709960938\n",
      "epoch : 39, loss: 0.1783277541399002\n",
      "epoch : 40, loss: 0.17973405122756958\n",
      "epoch : 41, loss: 0.18337412178516388\n",
      "epoch : 42, loss: 0.1834661141037941\n",
      "epoch : 43, loss: 0.1830315813422203\n",
      "epoch : 44, loss: 0.17545976489782333\n",
      "epoch : 45, loss: 0.17512046545743942\n",
      "epoch : 46, loss: 0.17660530656576157\n",
      "epoch : 47, loss: 0.18022044003009796\n",
      "epoch : 48, loss: 0.18104510009288788\n",
      "epoch : 49, loss: 0.1842109113931656\n",
      "epoch : 50, loss: 0.17443298548460007\n",
      "epoch : 51, loss: 0.17894048988819122\n",
      "epoch : 52, loss: 0.17747455090284348\n",
      "epoch : 53, loss: 0.1756826862692833\n",
      "epoch : 54, loss: 0.17420481890439987\n",
      "epoch : 55, loss: 0.17104102671146393\n",
      "epoch : 56, loss: 0.17655189335346222\n",
      "epoch : 57, loss: 0.16771574318408966\n",
      "epoch : 58, loss: 0.17061196267604828\n",
      "epoch : 59, loss: 0.1759457290172577\n",
      "epoch : 60, loss: 0.16971591114997864\n",
      "epoch : 61, loss: 0.17008288204669952\n",
      "epoch : 62, loss: 0.16810516268014908\n",
      "epoch : 63, loss: 0.1718168631196022\n",
      "epoch : 64, loss: 0.16651476919651031\n",
      "epoch : 65, loss: 0.16905046999454498\n",
      "epoch : 66, loss: 0.16839753836393356\n",
      "epoch : 67, loss: 0.17026201635599136\n",
      "epoch : 68, loss: 0.16833332180976868\n",
      "epoch : 69, loss: 0.17000722140073776\n",
      "epoch : 70, loss: 0.1660449579358101\n",
      "epoch : 71, loss: 0.16872187703847885\n",
      "epoch : 72, loss: 0.1711898297071457\n",
      "epoch : 73, loss: 0.1646982952952385\n",
      "epoch : 74, loss: 0.17654716968536377\n",
      "epoch : 75, loss: 0.1635541394352913\n",
      "epoch : 76, loss: 0.1673773154616356\n",
      "epoch : 77, loss: 0.17275308072566986\n",
      "epoch : 78, loss: 0.1660223826766014\n",
      "epoch : 79, loss: 0.17343437671661377\n",
      "epoch : 80, loss: 0.16676562279462814\n",
      "epoch : 81, loss: 0.16632917523384094\n",
      "epoch : 82, loss: 0.16407299041748047\n",
      "epoch : 83, loss: 0.1592448204755783\n",
      "epoch : 84, loss: 0.16614656150341034\n",
      "epoch : 85, loss: 0.16595938056707382\n",
      "epoch : 86, loss: 0.1662048175930977\n",
      "epoch : 87, loss: 0.1624334380030632\n",
      "epoch : 88, loss: 0.16529978066682816\n",
      "epoch : 89, loss: 0.16254114359617233\n",
      "epoch : 90, loss: 0.1656261384487152\n",
      "epoch : 91, loss: 0.16113460808992386\n",
      "epoch : 92, loss: 0.1598258912563324\n",
      "epoch : 93, loss: 0.16677484661340714\n",
      "epoch : 94, loss: 0.16927077621221542\n",
      "epoch : 95, loss: 0.1709323152899742\n",
      "epoch : 96, loss: 0.1683562621474266\n",
      "epoch : 97, loss: 0.17120008915662766\n",
      "epoch : 98, loss: 0.16683804243803024\n",
      "epoch : 99, loss: 0.16701263189315796\n",
      "Finished Training\n",
      "epoch : 0, loss: 0.9444168210029602\n",
      "epoch : 1, loss: 0.9834215044975281\n",
      "epoch : 2, loss: 0.9240589737892151\n",
      "epoch : 3, loss: 0.8758253753185272\n",
      "epoch : 4, loss: 0.8900404870510101\n",
      "epoch : 5, loss: 0.8739126622676849\n",
      "epoch : 6, loss: 0.8628820478916168\n",
      "epoch : 7, loss: 0.8332781195640564\n",
      "epoch : 8, loss: 0.8383786976337433\n",
      "epoch : 9, loss: 0.8430845737457275\n",
      "epoch : 10, loss: 0.8303636312484741\n",
      "epoch : 11, loss: 0.8136737048625946\n",
      "epoch : 12, loss: 0.7369356155395508\n",
      "epoch : 13, loss: 0.7289066910743713\n",
      "epoch : 14, loss: 0.6968861818313599\n",
      "epoch : 15, loss: 0.5847814381122589\n",
      "epoch : 16, loss: 0.5314781069755554\n",
      "epoch : 17, loss: 0.464647576212883\n",
      "epoch : 18, loss: 0.42981840670108795\n",
      "epoch : 19, loss: 0.4154156595468521\n",
      "epoch : 20, loss: 0.39106030762195587\n",
      "epoch : 21, loss: 0.36261625587940216\n",
      "epoch : 22, loss: 0.3379388004541397\n",
      "epoch : 23, loss: 0.31551623344421387\n",
      "epoch : 24, loss: 0.30232833325862885\n",
      "epoch : 25, loss: 0.2810892015695572\n",
      "epoch : 26, loss: 0.27330951392650604\n",
      "epoch : 27, loss: 0.26091139018535614\n",
      "epoch : 28, loss: 0.24900200963020325\n",
      "epoch : 29, loss: 0.23671413213014603\n",
      "epoch : 30, loss: 0.23442212492227554\n",
      "epoch : 31, loss: 0.22507037967443466\n",
      "epoch : 32, loss: 0.21810806542634964\n",
      "epoch : 33, loss: 0.2130586877465248\n",
      "epoch : 34, loss: 0.2098148837685585\n",
      "epoch : 35, loss: 0.2086166962981224\n",
      "epoch : 36, loss: 0.203828364610672\n",
      "epoch : 37, loss: 0.19988670945167542\n",
      "epoch : 38, loss: 0.19862163066864014\n",
      "epoch : 39, loss: 0.1962074637413025\n",
      "epoch : 40, loss: 0.19064247608184814\n",
      "epoch : 41, loss: 0.18942522257566452\n",
      "epoch : 42, loss: 0.18759527802467346\n",
      "epoch : 43, loss: 0.187876395881176\n",
      "epoch : 44, loss: 0.1830175668001175\n",
      "epoch : 45, loss: 0.18587641417980194\n",
      "epoch : 46, loss: 0.18248837441205978\n",
      "epoch : 47, loss: 0.18220876902341843\n",
      "epoch : 48, loss: 0.18226709961891174\n",
      "epoch : 49, loss: 0.1817157045006752\n",
      "epoch : 50, loss: 0.17933306843042374\n",
      "epoch : 51, loss: 0.17953399568796158\n",
      "epoch : 52, loss: 0.18199161440134048\n",
      "epoch : 53, loss: 0.18101347237825394\n",
      "epoch : 54, loss: 0.17847362160682678\n",
      "epoch : 55, loss: 0.17683292925357819\n",
      "epoch : 56, loss: 0.17611724883317947\n",
      "epoch : 57, loss: 0.1789858639240265\n",
      "epoch : 58, loss: 0.17932692170143127\n",
      "epoch : 59, loss: 0.17604942619800568\n",
      "epoch : 60, loss: 0.17651551216840744\n",
      "epoch : 61, loss: 0.17679046839475632\n",
      "epoch : 62, loss: 0.17428311705589294\n",
      "epoch : 63, loss: 0.1774752140045166\n",
      "epoch : 64, loss: 0.17505118995904922\n",
      "epoch : 65, loss: 0.17499275505542755\n",
      "epoch : 66, loss: 0.17560189962387085\n",
      "epoch : 67, loss: 0.17482034862041473\n",
      "epoch : 68, loss: 0.17522604763507843\n",
      "epoch : 69, loss: 0.17559319734573364\n",
      "epoch : 70, loss: 0.17237497121095657\n",
      "epoch : 71, loss: 0.17301127314567566\n",
      "epoch : 72, loss: 0.17633354663848877\n",
      "epoch : 73, loss: 0.17284895479679108\n",
      "epoch : 74, loss: 0.17476923763751984\n",
      "epoch : 75, loss: 0.17241740971803665\n",
      "epoch : 76, loss: 0.1739874705672264\n",
      "epoch : 77, loss: 0.1733163371682167\n",
      "epoch : 78, loss: 0.17263548076152802\n",
      "epoch : 79, loss: 0.17225731909275055\n",
      "epoch : 80, loss: 0.17283928394317627\n",
      "epoch : 81, loss: 0.17314308881759644\n",
      "epoch : 82, loss: 0.17196918278932571\n",
      "epoch : 83, loss: 0.17509472370147705\n",
      "epoch : 84, loss: 0.17161482572555542\n",
      "epoch : 85, loss: 0.17163357138633728\n",
      "epoch : 86, loss: 0.1712089627981186\n",
      "epoch : 87, loss: 0.17280536144971848\n",
      "epoch : 88, loss: 0.1734115108847618\n",
      "epoch : 89, loss: 0.17219090461730957\n",
      "epoch : 90, loss: 0.17267287522554398\n",
      "epoch : 91, loss: 0.17169464379549026\n",
      "epoch : 92, loss: 0.1741914004087448\n",
      "epoch : 93, loss: 0.17205476015806198\n",
      "epoch : 94, loss: 0.1737866923213005\n",
      "epoch : 95, loss: 0.17262231558561325\n",
      "epoch : 96, loss: 0.17231861501932144\n",
      "epoch : 97, loss: 0.1721392124891281\n",
      "epoch : 98, loss: 0.17108223587274551\n",
      "epoch : 99, loss: 0.17203760147094727\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "encoded_es_grr_list = train(es_grr_list)\n",
    "encoded_es_dol_list = train(es_dol_list)\n",
    "encoded_es_sg_list = train(es_sg_list)\n",
    "\n",
    "encoded_rps_grr_list = train(rps_grr_list)\n",
    "encoded_rps_dol_list = train(rps_dol_list)\n",
    "encoded_rps_sg_list = train(rps_sg_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e48b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-24.9774, -31.3812, -26.6797, -24.6731, -21.9418, -20.0058, -19.4663,\n",
      "        -17.5291, -15.9381, -14.8090, -13.5477, -12.1601, -11.3389,  -9.2548,\n",
      "         -7.9598,  -6.4812, -31.1909, -35.4601, -31.4720, -28.8491, -23.3081,\n",
      "        -21.2620, -17.9917, -14.3676, -11.6996, -10.2890,  -9.6853,  -8.3825,\n",
      "         -6.4104,  -4.2516,  -3.9875,  -2.5728], device='cuda:0')\n",
      "tensor([-40.4090, -42.7548, -37.5884, -35.8237, -32.7759, -31.5517, -30.7348,\n",
      "        -29.1994, -27.5940, -25.1336, -23.3887, -19.8454, -18.1463, -17.0682,\n",
      "        -14.2170, -10.4736, -44.7814, -49.0910, -43.5504, -39.1361, -35.2983,\n",
      "        -32.1538, -28.2776, -24.5757, -21.2351, -18.6618, -18.6314, -11.7865,\n",
      "        -15.0560, -13.4057,  -7.2478,  -7.9574], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "idx = 16\n",
    "print(encoded_es_grr_list[idx])\n",
    "print(encoded_es_sg_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b75927e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "es_concated = encoded_es_grr_list +  encoded_es_sg_list +  encoded_es_dol_list\n",
    "es_concated = torch.stack(es_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "es_cluster_labels = kmeans.fit_predict(es_concated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f489882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "98\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_es_grr_list))\n",
    "print(len(encoded_es_sg_list))\n",
    "print(len(encoded_es_dol_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3aa87fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 36, 1: 0, 2: 2}\n",
      "{0: 27, 1: 0, 2: 71}\n",
      "{0: 0, 1: 107, 2: 0}\n"
     ]
    }
   ],
   "source": [
    "es_clstr_grr_cnt={0:0, 1:0, 2:0} \n",
    "es_clstr_sg_cnt={0:0, 1:0, 2:0} \n",
    "es_clstr_dol_cnt={0:0, 1:0, 2:0} \n",
    "\n",
    "for i, cl_label in enumerate(es_cluster_labels):\n",
    "    if i < len(encoded_es_grr_list):\n",
    "        es_clstr_grr_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_es_grr_list) + len(encoded_es_sg_list):\n",
    "        es_clstr_sg_cnt[cl_label]+= 1\n",
    "    else:\n",
    "        es_clstr_dol_cnt[cl_label]+= 1\n",
    "\n",
    "      \n",
    "print(es_clstr_grr_cnt)\n",
    "print(es_clstr_sg_cnt)\n",
    "print(es_clstr_dol_cnt) # grrsg, dol 의 필요성. 모비스에선 grrdol, sg라고 했는데. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "133c39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 38, 1: 107}\n",
      "{0: 98, 1: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "es_clstr_grrdol_cnt={0:0, 1:0} \n",
    "es_clstr_ssg_cnt={0:0, 1:0} \n",
    "\n",
    "_2kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "_2es_cluster_labels = _2kmeans.fit_predict(es_concated)\n",
    "\n",
    "for i, cl_label in enumerate(_2es_cluster_labels):\n",
    "    if i < len(encoded_es_grr_list):\n",
    "        es_clstr_grrdol_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_es_grr_list) + len(encoded_es_sg_list) :\n",
    "        es_clstr_ssg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        es_clstr_grrdol_cnt[cl_label] +=1\n",
    "\n",
    "print(es_clstr_grrdol_cnt)\n",
    "print(es_clstr_ssg_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57f6b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 136, 1: 0}\n",
      "{0: 0, 1: 107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "es_clstr_grrsg_cnt={0:0, 1:0} \n",
    "es_clstr_dol_cnt={0:0, 1:0} \n",
    "\n",
    "_2es_cluster_labels = _2kmeans.fit_predict(es_concated)\n",
    "\n",
    "for i, cl_label in enumerate(_2es_cluster_labels):\n",
    "    if i < len(encoded_es_grr_list):\n",
    "        es_clstr_grrsg_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_es_grr_list) + len(encoded_es_sg_list) :\n",
    "        es_clstr_grrsg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        es_clstr_dol_cnt[cl_label] +=1\n",
    "\n",
    "print(es_clstr_grrsg_cnt)\n",
    "print(es_clstr_dol_cnt) # grrsg | dol 이 잘된다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ac2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245db8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "951db663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "46\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_rps_grr_list))\n",
    "print(len(encoded_rps_sg_list))\n",
    "print(len(encoded_rps_dol_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ccc4d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 49, 1: 10}\n",
      "{0: 0, 1: 43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "rps_concated = encoded_rps_grr_list +  encoded_rps_sg_list +  encoded_rps_dol_list\n",
    "rps_concated = torch.stack(rps_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "rps_cluster_labels = _2kmeans.fit_predict(rps_concated)\n",
    "\n",
    "rps_clstr_grrsg_cnt={0:0, 1:0} \n",
    "rps_clstr_dol_cnt={0:0, 1:0} \n",
    "\n",
    "for i, cl_label in enumerate(rps_cluster_labels):\n",
    "    if i < len(encoded_rps_grr_list):\n",
    "        rps_clstr_grrsg_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_rps_grr_list) + len(encoded_rps_sg_list) :\n",
    "        rps_clstr_grrsg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        rps_clstr_dol_cnt[cl_label] +=1\n",
    "\n",
    "print(rps_clstr_grrsg_cnt)\n",
    "print(rps_clstr_dol_cnt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8d95289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3, 1: 53}\n",
      "{0: 46, 1: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Run k-means clustering\n",
    "rps_cluster_labels = _2kmeans.fit_predict(rps_concated)\n",
    "\n",
    "rps_clstr_grrdol_cnt={0:0, 1:0} \n",
    "rps_clstr_sg_cnt={0:0, 1:0} \n",
    "\n",
    "for i, cl_label in enumerate(rps_cluster_labels):\n",
    "    if i < len(encoded_rps_grr_list):\n",
    "        rps_clstr_grrdol_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_rps_grr_list) + len(encoded_rps_sg_list) :\n",
    "        rps_clstr_sg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        rps_clstr_grrdol_cnt[cl_label] +=1\n",
    "\n",
    "print(rps_clstr_grrdol_cnt)\n",
    "print(rps_clstr_sg_cnt)   # 여기선 또 grrdol | sg 이 잘된다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64163d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 13, 1: 0, 2: 0}\n",
      "{0: 0, 1: 46, 2: 0}\n",
      "{0: 20, 1: 0, 2: 23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "rps_concated = encoded_rps_grr_list +  encoded_rps_sg_list +  encoded_rps_dol_list\n",
    "rps_concated = torch.stack(rps_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "rps_cluster_labels = kmeans.fit_predict(rps_concated)\n",
    "\n",
    "rps_clstr_grr_cnt={0:0, 1:0, 2:0} # total should be: 298\n",
    "rps_clstr_sg_cnt={0:0, 1:0, 2:0} # total should be: 298\n",
    "rps_clstr_dol_cnt={0:0, 1:0, 2:0} # total should be: 298\n",
    "\n",
    "for i, cl_label in enumerate(rps_cluster_labels):\n",
    "    if i < len(encoded_rps_grr_list):\n",
    "        rps_clstr_grr_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_rps_grr_list) + len(encoded_rps_sg_list) :\n",
    "        rps_clstr_sg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        rps_clstr_dol_cnt[cl_label] +=1\n",
    "\n",
    "print(rps_clstr_grr_cnt)\n",
    "print(rps_clstr_sg_cnt)\n",
    "print(rps_clstr_dol_cnt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cb90eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_rps_sg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9761f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de2c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d1a26cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/librosa/core/convert.py:1870: RuntimeWarning: divide by zero encountered in log10\n",
      "  + 2 * np.log10(f_sq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97, 352])\n",
      "torch.Size([97, 355])\n",
      "torch.Size([97, 357])\n",
      "torch.Size([97, 368])\n"
     ]
    }
   ],
   "source": [
    "line_df = pd.read_csv('LINE_meta.csv')\n",
    "\n",
    "line_grr_list = []\n",
    "line_sg_list = []\n",
    "line_dol_list = []\n",
    "\n",
    "for idx in line_df.index:\n",
    "    row = line_df.loc[idx]\n",
    "    filename= row['filename']\n",
    "    if row['category'] == 'GRR':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=400:\n",
    "            line_grr_list.append(ten[:,:400].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "\n",
    "    elif row['category'] == 'SG':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=400: # 400으로?\n",
    "            line_sg_list.append(ten[:,:400].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "\n",
    "    elif row['category'] == 'DOL':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=400:\n",
    "            line_dol_list.append(ten[:,:400].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4c1a699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97, 400])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_grr_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55788577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class line_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(line_Autoencoder, self).__init__() # (97, 400) 이라고 했을 때 \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential( # Given groups=1, weight of size [16, 128, 3, 3], expected input[1, 32, 8, 95] to have 128 channels, but got 32 channels instead\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 49, 200)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 25, 100)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 64, 13, 50)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 7, 25)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 4, 13)\n",
    "            \n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 16, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,0)),  # Output shape: (b, 16, 7, 25)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2, padding=(1, 1), output_padding=(0,1)),  # Output shape: (b, 32, 13, 50) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 64, 25, 100) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 32, 49, 200)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 16, 97, 400)  \n",
    "            \n",
    "        )\n",
    "# output padding must be smaller than stride\n",
    "    def forward(self, x):\n",
    "        mid = self.encoder(x)\n",
    "        x = self.decoder(mid)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59fcba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_train(dataset_list):\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    customdataset = CustomDataset(dataset_list)\n",
    "    train_loader = DataLoader(customdataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    autoencoder = line_Autoencoder().to(dev)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader): # i-th batch\n",
    "            inputs = data\n",
    "            inputs = inputs.to(dev)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'epoch : {epoch}, loss: {running_loss}')\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    encoded_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in dataset_list:\n",
    "            encoded_list.append(autoencoder.encoder(i.to(dev)).flatten())\n",
    "\n",
    "    return encoded_list\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f58a2ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss: 1.762221336364746\n",
      "epoch : 1, loss: 1.7548028230667114\n",
      "epoch : 2, loss: 1.7473225593566895\n",
      "epoch : 3, loss: 1.7397414445877075\n",
      "epoch : 4, loss: 1.732003927230835\n",
      "epoch : 5, loss: 1.7241140604019165\n",
      "epoch : 6, loss: 1.716019868850708\n",
      "epoch : 7, loss: 1.7076514959335327\n",
      "epoch : 8, loss: 1.6989134550094604\n",
      "epoch : 9, loss: 1.6896555423736572\n",
      "epoch : 10, loss: 1.6796834468841553\n",
      "epoch : 11, loss: 1.6690305471420288\n",
      "epoch : 12, loss: 1.656985878944397\n",
      "epoch : 13, loss: 1.6423554420471191\n",
      "epoch : 14, loss: 1.6235017776489258\n",
      "epoch : 15, loss: 1.5980122089385986\n",
      "epoch : 16, loss: 1.562334418296814\n",
      "epoch : 17, loss: 1.511401653289795\n",
      "epoch : 18, loss: 1.4379178285598755\n",
      "epoch : 19, loss: 1.3332713842391968\n",
      "epoch : 20, loss: 1.1909265518188477\n",
      "epoch : 21, loss: 1.0160973072052002\n",
      "epoch : 22, loss: 0.855827808380127\n",
      "epoch : 23, loss: 0.8524380922317505\n",
      "epoch : 24, loss: 0.9180907607078552\n",
      "epoch : 25, loss: 0.8029083013534546\n",
      "epoch : 26, loss: 0.6627845168113708\n",
      "epoch : 27, loss: 0.5900957584381104\n",
      "epoch : 28, loss: 0.5678852200508118\n",
      "epoch : 29, loss: 0.5536811351776123\n",
      "epoch : 30, loss: 0.5238737463951111\n",
      "epoch : 31, loss: 0.4739924669265747\n",
      "epoch : 32, loss: 0.41334453225135803\n",
      "epoch : 33, loss: 0.3624371290206909\n",
      "epoch : 34, loss: 0.3438054919242859\n",
      "epoch : 35, loss: 0.35160383582115173\n",
      "epoch : 36, loss: 0.34234753251075745\n",
      "epoch : 37, loss: 0.304266095161438\n",
      "epoch : 38, loss: 0.26982560753822327\n",
      "epoch : 39, loss: 0.25802284479141235\n",
      "epoch : 40, loss: 0.25958001613616943\n",
      "epoch : 41, loss: 0.258870005607605\n",
      "epoch : 42, loss: 0.24883460998535156\n",
      "epoch : 43, loss: 0.23277096450328827\n",
      "epoch : 44, loss: 0.22014915943145752\n",
      "epoch : 45, loss: 0.2181406468153\n",
      "epoch : 46, loss: 0.2220146656036377\n",
      "epoch : 47, loss: 0.2194320410490036\n",
      "epoch : 48, loss: 0.20909565687179565\n",
      "epoch : 49, loss: 0.20069970190525055\n",
      "epoch : 50, loss: 0.1991880238056183\n",
      "epoch : 51, loss: 0.20042972266674042\n",
      "epoch : 52, loss: 0.19875521957874298\n",
      "epoch : 53, loss: 0.19324825704097748\n",
      "epoch : 54, loss: 0.18752872943878174\n",
      "epoch : 55, loss: 0.18537726998329163\n",
      "epoch : 56, loss: 0.18595758080482483\n",
      "epoch : 57, loss: 0.18472839891910553\n",
      "epoch : 58, loss: 0.18031971156597137\n",
      "epoch : 59, loss: 0.17614373564720154\n",
      "epoch : 60, loss: 0.1745813488960266\n",
      "epoch : 61, loss: 0.17417287826538086\n",
      "epoch : 62, loss: 0.17252066731452942\n",
      "epoch : 63, loss: 0.1693830043077469\n",
      "epoch : 64, loss: 0.16661037504673004\n",
      "epoch : 65, loss: 0.16553948819637299\n",
      "epoch : 66, loss: 0.16505005955696106\n",
      "epoch : 67, loss: 0.16333851218223572\n",
      "epoch : 68, loss: 0.1608254760503769\n",
      "epoch : 69, loss: 0.1592351198196411\n",
      "epoch : 70, loss: 0.1585935652256012\n",
      "epoch : 71, loss: 0.15751910209655762\n",
      "epoch : 72, loss: 0.15567293763160706\n",
      "epoch : 73, loss: 0.15396201610565186\n",
      "epoch : 74, loss: 0.1529357135295868\n",
      "epoch : 75, loss: 0.1519847959280014\n",
      "epoch : 76, loss: 0.1505260467529297\n",
      "epoch : 77, loss: 0.14899525046348572\n",
      "epoch : 78, loss: 0.14798495173454285\n",
      "epoch : 79, loss: 0.14718297123908997\n",
      "epoch : 80, loss: 0.14592838287353516\n",
      "epoch : 81, loss: 0.14458081126213074\n",
      "epoch : 82, loss: 0.1437557190656662\n",
      "epoch : 83, loss: 0.1430223435163498\n",
      "epoch : 84, loss: 0.14194168150424957\n",
      "epoch : 85, loss: 0.14101548492908478\n",
      "epoch : 86, loss: 0.14046868681907654\n",
      "epoch : 87, loss: 0.13966628909111023\n",
      "epoch : 88, loss: 0.13878746330738068\n",
      "epoch : 89, loss: 0.13831806182861328\n",
      "epoch : 90, loss: 0.13769862055778503\n",
      "epoch : 91, loss: 0.13691261410713196\n",
      "epoch : 92, loss: 0.136464461684227\n",
      "epoch : 93, loss: 0.13593627512454987\n",
      "epoch : 94, loss: 0.13528990745544434\n",
      "epoch : 95, loss: 0.13493019342422485\n",
      "epoch : 96, loss: 0.13438591361045837\n",
      "epoch : 97, loss: 0.13386662304401398\n",
      "epoch : 98, loss: 0.1335282176733017\n",
      "epoch : 99, loss: 0.13303987681865692\n",
      "Finished Training\n",
      "epoch : 0, loss: 9.073832750320435\n",
      "epoch : 1, loss: 8.984317779541016\n",
      "epoch : 2, loss: 8.984417200088501\n",
      "epoch : 3, loss: 9.11850881576538\n",
      "epoch : 4, loss: 8.112164378166199\n",
      "epoch : 5, loss: 6.020004391670227\n",
      "epoch : 6, loss: 4.141998589038849\n",
      "epoch : 7, loss: 2.808731973171234\n",
      "epoch : 8, loss: 2.354945123195648\n",
      "epoch : 9, loss: 1.8930814266204834\n",
      "epoch : 10, loss: 1.349784642457962\n",
      "epoch : 11, loss: 1.2309078872203827\n",
      "epoch : 12, loss: 1.0639186203479767\n",
      "epoch : 13, loss: 0.9032074958086014\n",
      "epoch : 14, loss: 0.8754071593284607\n",
      "epoch : 15, loss: 0.8255873769521713\n",
      "epoch : 16, loss: 0.7776307761669159\n",
      "epoch : 17, loss: 0.7224310785531998\n",
      "epoch : 18, loss: 0.718178316950798\n",
      "epoch : 19, loss: 0.6789951175451279\n",
      "epoch : 20, loss: 0.6685501784086227\n",
      "epoch : 21, loss: 0.6307279318571091\n",
      "epoch : 22, loss: 0.6314920336008072\n",
      "epoch : 23, loss: 0.6012633293867111\n",
      "epoch : 24, loss: 0.5900708734989166\n",
      "epoch : 25, loss: 0.5813458859920502\n",
      "epoch : 26, loss: 0.5707805007696152\n",
      "epoch : 27, loss: 0.5718432813882828\n",
      "epoch : 28, loss: 0.5632724910974503\n",
      "epoch : 29, loss: 0.5561763197183609\n",
      "epoch : 30, loss: 0.5536180138587952\n",
      "epoch : 31, loss: 0.5420610904693604\n",
      "epoch : 32, loss: 0.5370925068855286\n",
      "epoch : 33, loss: 0.5350201427936554\n",
      "epoch : 34, loss: 0.5255116671323776\n",
      "epoch : 35, loss: 0.5303691327571869\n",
      "epoch : 36, loss: 0.5242713540792465\n",
      "epoch : 37, loss: 0.5188777893781662\n",
      "epoch : 38, loss: 0.5149399638175964\n",
      "epoch : 39, loss: 0.5187865793704987\n",
      "epoch : 40, loss: 0.501223161816597\n",
      "epoch : 41, loss: 0.503814272582531\n",
      "epoch : 42, loss: 0.49819640815258026\n",
      "epoch : 43, loss: 0.4997268319129944\n",
      "epoch : 44, loss: 0.49692970514297485\n",
      "epoch : 45, loss: 0.4953387752175331\n",
      "epoch : 46, loss: 0.483462929725647\n",
      "epoch : 47, loss: 0.4858065992593765\n",
      "epoch : 48, loss: 0.4815126955509186\n",
      "epoch : 49, loss: 0.48478081822395325\n",
      "epoch : 50, loss: 0.4789438098669052\n",
      "epoch : 51, loss: 0.4755602106451988\n",
      "epoch : 52, loss: 0.4749041125178337\n",
      "epoch : 53, loss: 0.47161852568387985\n",
      "epoch : 54, loss: 0.4694588631391525\n",
      "epoch : 55, loss: 0.4723150134086609\n",
      "epoch : 56, loss: 0.4713835269212723\n",
      "epoch : 57, loss: 0.46614640951156616\n",
      "epoch : 58, loss: 0.4679955542087555\n",
      "epoch : 59, loss: 0.4643497169017792\n",
      "epoch : 60, loss: 0.46221770346164703\n",
      "epoch : 61, loss: 0.46651453524827957\n",
      "epoch : 62, loss: 0.46494659781455994\n",
      "epoch : 63, loss: 0.459814615547657\n",
      "epoch : 64, loss: 0.45848891139030457\n",
      "epoch : 65, loss: 0.4598323255777359\n",
      "epoch : 66, loss: 0.4583963081240654\n",
      "epoch : 67, loss: 0.4577418491244316\n",
      "epoch : 68, loss: 0.46244315803050995\n",
      "epoch : 69, loss: 0.45617470145225525\n",
      "epoch : 70, loss: 0.4552883952856064\n",
      "epoch : 71, loss: 0.4558119997382164\n",
      "epoch : 72, loss: 0.45307520776987076\n",
      "epoch : 73, loss: 0.4558515325188637\n",
      "epoch : 74, loss: 0.4519801214337349\n",
      "epoch : 75, loss: 0.4486028105020523\n",
      "epoch : 76, loss: 0.45118457078933716\n",
      "epoch : 77, loss: 0.4515133798122406\n",
      "epoch : 78, loss: 0.44827212393283844\n",
      "epoch : 79, loss: 0.4462812915444374\n",
      "epoch : 80, loss: 0.4461151212453842\n",
      "epoch : 81, loss: 0.44628316164016724\n",
      "epoch : 82, loss: 0.4462379142642021\n",
      "epoch : 83, loss: 0.4454466849565506\n",
      "epoch : 84, loss: 0.44820860028266907\n",
      "epoch : 85, loss: 0.44894061982631683\n",
      "epoch : 86, loss: 0.4480496942996979\n",
      "epoch : 87, loss: 0.4475671797990799\n",
      "epoch : 88, loss: 0.4450017511844635\n",
      "epoch : 89, loss: 0.44506315141916275\n",
      "epoch : 90, loss: 0.44292663037776947\n",
      "epoch : 91, loss: 0.44695624709129333\n",
      "epoch : 92, loss: 0.44015078991651535\n",
      "epoch : 93, loss: 0.4447697401046753\n",
      "epoch : 94, loss: 0.4407021030783653\n",
      "epoch : 95, loss: 0.4400069937109947\n",
      "epoch : 96, loss: 0.43919018656015396\n",
      "epoch : 97, loss: 0.44070812314748764\n",
      "epoch : 98, loss: 0.4391447752714157\n",
      "epoch : 99, loss: 0.44265084713697433\n",
      "Finished Training\n",
      "epoch : 0, loss: 10.662584781646729\n",
      "epoch : 1, loss: 10.27128791809082\n",
      "epoch : 2, loss: 10.737154245376587\n",
      "epoch : 3, loss: 10.119164228439331\n",
      "epoch : 4, loss: 8.346247553825378\n",
      "epoch : 5, loss: 6.503830313682556\n",
      "epoch : 6, loss: 4.47479373216629\n",
      "epoch : 7, loss: 3.608711540699005\n",
      "epoch : 8, loss: 2.6228495836257935\n",
      "epoch : 9, loss: 1.7478505969047546\n",
      "epoch : 10, loss: 1.3656392693519592\n",
      "epoch : 11, loss: 1.1273780465126038\n",
      "epoch : 12, loss: 0.9960833489894867\n",
      "epoch : 13, loss: 0.8823518306016922\n",
      "epoch : 14, loss: 0.8115607500076294\n",
      "epoch : 15, loss: 0.7966173440217972\n",
      "epoch : 16, loss: 0.7686700075864792\n",
      "epoch : 17, loss: 0.6959147602319717\n",
      "epoch : 18, loss: 0.675224244594574\n",
      "epoch : 19, loss: 0.6392875909805298\n",
      "epoch : 20, loss: 0.6632798165082932\n",
      "epoch : 21, loss: 0.6124618649482727\n",
      "epoch : 22, loss: 0.6079121828079224\n",
      "epoch : 23, loss: 0.6239507049322128\n",
      "epoch : 24, loss: 0.5825493335723877\n",
      "epoch : 25, loss: 0.5939865708351135\n",
      "epoch : 26, loss: 0.5487862527370453\n",
      "epoch : 27, loss: 0.5355748310685158\n",
      "epoch : 28, loss: 0.5414971858263016\n",
      "epoch : 29, loss: 0.5235343426465988\n",
      "epoch : 30, loss: 0.5234165862202644\n",
      "epoch : 31, loss: 0.5317341834306717\n",
      "epoch : 32, loss: 0.5200995802879333\n",
      "epoch : 33, loss: 0.5135175958275795\n",
      "epoch : 34, loss: 0.4964991584420204\n",
      "epoch : 35, loss: 0.49761340767145157\n",
      "epoch : 36, loss: 0.4890592619776726\n",
      "epoch : 37, loss: 0.4968976154923439\n",
      "epoch : 38, loss: 0.4877922758460045\n",
      "epoch : 39, loss: 0.49036024510860443\n",
      "epoch : 40, loss: 0.48937513679265976\n",
      "epoch : 41, loss: 0.47535908967256546\n",
      "epoch : 42, loss: 0.47309698164463043\n",
      "epoch : 43, loss: 0.48324189335107803\n",
      "epoch : 44, loss: 0.4864290654659271\n",
      "epoch : 45, loss: 0.47876910120248795\n",
      "epoch : 46, loss: 0.469402439892292\n",
      "epoch : 47, loss: 0.46486566960811615\n",
      "epoch : 48, loss: 0.46290721744298935\n",
      "epoch : 49, loss: 0.46258100867271423\n",
      "epoch : 50, loss: 0.4589935466647148\n",
      "epoch : 51, loss: 0.4613492488861084\n",
      "epoch : 52, loss: 0.45677579939365387\n",
      "epoch : 53, loss: 0.4666185826063156\n",
      "epoch : 54, loss: 0.4565949812531471\n",
      "epoch : 55, loss: 0.4600362330675125\n",
      "epoch : 56, loss: 0.4542033448815346\n",
      "epoch : 57, loss: 0.45511477440595627\n",
      "epoch : 58, loss: 0.4502706751227379\n",
      "epoch : 59, loss: 0.4547993689775467\n",
      "epoch : 60, loss: 0.456660658121109\n",
      "epoch : 61, loss: 0.45208006352186203\n",
      "epoch : 62, loss: 0.44963551312685013\n",
      "epoch : 63, loss: 0.44736889004707336\n",
      "epoch : 64, loss: 0.4485718831419945\n",
      "epoch : 65, loss: 0.4473663568496704\n",
      "epoch : 66, loss: 0.4436362609267235\n",
      "epoch : 67, loss: 0.44303061068058014\n",
      "epoch : 68, loss: 0.4384235739707947\n",
      "epoch : 69, loss: 0.4474922716617584\n",
      "epoch : 70, loss: 0.438711479306221\n",
      "epoch : 71, loss: 0.4404138922691345\n",
      "epoch : 72, loss: 0.4420766979455948\n",
      "epoch : 73, loss: 0.437826469540596\n",
      "epoch : 74, loss: 0.44316810369491577\n",
      "epoch : 75, loss: 0.44843604415655136\n",
      "epoch : 76, loss: 0.44605902582407\n",
      "epoch : 77, loss: 0.4407767429947853\n",
      "epoch : 78, loss: 0.4452209770679474\n",
      "epoch : 79, loss: 0.4405352920293808\n",
      "epoch : 80, loss: 0.44371654093265533\n",
      "epoch : 81, loss: 0.43665050715208054\n",
      "epoch : 82, loss: 0.429303340613842\n",
      "epoch : 83, loss: 0.439624086022377\n",
      "epoch : 84, loss: 0.43160219490528107\n",
      "epoch : 85, loss: 0.44055332243442535\n",
      "epoch : 86, loss: 0.4341385066509247\n",
      "epoch : 87, loss: 0.43440163135528564\n",
      "epoch : 88, loss: 0.4333409070968628\n",
      "epoch : 89, loss: 0.4349793270230293\n",
      "epoch : 90, loss: 0.42632223665714264\n",
      "epoch : 91, loss: 0.431444987654686\n",
      "epoch : 92, loss: 0.42785219848155975\n",
      "epoch : 93, loss: 0.43275442719459534\n",
      "epoch : 94, loss: 0.427324615418911\n",
      "epoch : 95, loss: 0.42999763786792755\n",
      "epoch : 96, loss: 0.4318750724196434\n",
      "epoch : 97, loss: 0.430792473256588\n",
      "epoch : 98, loss: 0.4278826043009758\n",
      "epoch : 99, loss: 0.43371954560279846\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "encoded_line_grr_list = line_train(line_grr_list)\n",
    "encoded_line_sg_list = line_train(line_sg_list)\n",
    "encoded_line_dol_list = line_train(line_dol_list)\n",
    "\n",
    "\n",
    "line_concated = encoded_line_grr_list +  encoded_line_sg_list +  encoded_line_dol_list\n",
    "line_concated = torch.stack(line_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "line_cluster_labels = kmeans.fit_predict(line_concated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "937362d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 23, 1: 0, 2: 1}\n",
      "{0: 12, 1: 0, 2: 94}\n",
      "{0: 0, 1: 102, 2: 0}\n"
     ]
    }
   ],
   "source": [
    "line_concated = encoded_line_grr_list +  encoded_line_sg_list +  encoded_line_dol_list\n",
    "line_concated = torch.stack(line_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "line_cluster_labels = kmeans.fit_predict(line_concated)\n",
    "\n",
    "line_clstr_grr_cnt={0:0, 1:0, 2:0} # total should be: 298\n",
    "line_clstr_sg_cnt={0:0, 1:0, 2:0} # total should be: 298\n",
    "line_clstr_dol_cnt={0:0, 1:0, 2:0} # total should be: 298\n",
    "\n",
    "for i, cl_label in enumerate(line_cluster_labels):\n",
    "    if i < len(encoded_line_grr_list):\n",
    "        line_clstr_grr_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_line_grr_list) + len(encoded_line_sg_list) :\n",
    "        line_clstr_sg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        line_clstr_dol_cnt[cl_label] +=1\n",
    "\n",
    "print(line_clstr_grr_cnt)\n",
    "print(line_clstr_sg_cnt)\n",
    "print(line_clstr_dol_cnt)  # line에서는, grrsg | dol \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b267e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "106\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_line_grr_list))\n",
    "print(len(encoded_line_sg_list))\n",
    "print(len(encoded_line_dol_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32d75ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 130, 1: 0}\n",
      "{0: 0, 1: 102}\n"
     ]
    }
   ],
   "source": [
    "line_clstr_grrsg_cnt={0:0, 1:0} \n",
    "line_clstr_dol_cnt={0:0, 1:0} \n",
    "\n",
    "_2line_cluster_labels = _2kmeans.fit_predict(line_concated)\n",
    "\n",
    "for i, cl_label in enumerate(_2line_cluster_labels):\n",
    "    if i < len(encoded_line_grr_list):\n",
    "        line_clstr_grrsg_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_line_grr_list) + len(encoded_line_sg_list) :\n",
    "        line_clstr_grrsg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        line_clstr_dol_cnt[cl_label] +=1\n",
    "\n",
    "print(line_clstr_grrsg_cnt)\n",
    "print(line_clstr_dol_cnt) # grrsg | dol 이 잘된다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68c7bdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 24, 1: 102}\n",
      "{0: 106, 1: 0}\n"
     ]
    }
   ],
   "source": [
    "line_clstr_grrdol_cnt={0:0, 1:0} \n",
    "line_clstr_sg_cnt={0:0, 1:0} \n",
    "\n",
    "_2line_cluster_labels = _2kmeans.fit_predict(line_concated)\n",
    "\n",
    "for i, cl_label in enumerate(_2line_cluster_labels):\n",
    "    if i < len(encoded_line_grr_list):\n",
    "        line_clstr_grrdol_cnt[cl_label]+= 1\n",
    "    elif i < len(encoded_line_grr_list) + len(encoded_line_sg_list) :\n",
    "        line_clstr_sg_cnt[cl_label] += 1\n",
    "    else:\n",
    "        line_clstr_grrdol_cnt[cl_label] +=1\n",
    "\n",
    "print(line_clstr_grrdol_cnt)\n",
    "print(line_clstr_sg_cnt) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6fd43e",
   "metadata": {},
   "source": [
    "결론: rps만 빼면 모두 grrsg | dol 이 가장 좋았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe01cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
