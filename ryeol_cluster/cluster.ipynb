{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_1/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_2/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_1/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_2/#01_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/wav_정상품_V1a/#02_SX2_Normal_ES_LH_1/#02_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>./data/wav_문제품DATAa/#44_SX2_C_ES_LH_1/#44_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_ES_RH_1/#33_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_ES_RH_2/#33_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_ES_RH_1/#21_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_ES_RH_2/#21_SX2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  fold  target category\n",
       "0    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_1/#01_...     1       0   Normal\n",
       "1    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_RH_2/#01_...     1       0   Normal\n",
       "2    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_1/#01_...     1       0   Normal\n",
       "3    ./data/wav_정상품_V1a/#01_SX2_Normal_ES_LH_2/#01_...     1       0   Normal\n",
       "4    ./data/wav_정상품_V1a/#02_SX2_Normal_ES_LH_1/#02_...     1       0   Normal\n",
       "..                                                 ...   ...     ...      ...\n",
       "540  ./data/wav_문제품DATAa/#44_SX2_C_ES_LH_1/#44_SX2_...     1       3       SG\n",
       "541  ./data/wav_문제품DATAa/#33_SX2_B_ES_RH_1/#33_SX2_...     1       3       SG\n",
       "542  ./data/wav_문제품DATAa/#33_SX2_B_ES_RH_2/#33_SX2_...     1       3       SG\n",
       "543  ./data/wav_문제품DATAa/#21_SX2_A_ES_RH_1/#21_SX2_...     1       3       SG\n",
       "544  ./data/wav_문제품DATAa/#21_SX2_A_ES_RH_2/#21_SX2_...     1       3       SG\n",
       "\n",
       "[545 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "es_df = pd.read_csv('ES_meta.csv')\n",
    "es_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff39da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_2/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_1/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_2/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_1/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/wav_정상품_V1a/#02_SX2_Normal_Line_RH_2/#0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>./data/wav_문제품DATAa/#32_SX2_A_LINE_LH_1/#32_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_2/#33_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_1/#33_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_2/#21_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_1/#21_SX...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  fold  target category\n",
       "0    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_2/#0...     1       0   Normal\n",
       "1    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_RH_1/#0...     1       0   Normal\n",
       "2    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_2/#0...     1       0   Normal\n",
       "3    ./data/wav_정상품_V1a/#01_SX2_Normal_Line_LH_1/#0...     1       0   Normal\n",
       "4    ./data/wav_정상품_V1a/#02_SX2_Normal_Line_RH_2/#0...     1       0   Normal\n",
       "..                                                 ...   ...     ...      ...\n",
       "545  ./data/wav_문제품DATAa/#32_SX2_A_LINE_LH_1/#32_SX...     1       3       SG\n",
       "546  ./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_2/#33_SX...     1       3       SG\n",
       "547  ./data/wav_문제품DATAa/#33_SX2_B_LINE_RH_1/#33_SX...     1       3       SG\n",
       "548  ./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_2/#21_SX...     1       3       SG\n",
       "549  ./data/wav_문제품DATAa/#21_SX2_A_LINE_RH_1/#21_SX...     1       3       SG\n",
       "\n",
       "[550 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_df = pd.read_csv('LINE_meta.csv')\n",
    "line_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e818fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "# .wav 파일 불러오기\n",
    "\n",
    "\n",
    "# 예제 사용\n",
    "\n",
    "def get_tensor(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)  # sr=None을 설정하여 원본 샘플링 레이트를 유지\n",
    "\n",
    "    # NumPy 배열을 PyTorch 텐서로 변환\n",
    "    tensor_data = torch.tensor(audio)\n",
    "\n",
    "    # 결과 확인\n",
    "    def perform_stft(waveform, n_fft=512, win_length=None, hop_length=None, window_fn=torch.hann_window):\n",
    "        stft = torch.stft(waveform, n_fft=n_fft, win_length=win_length, hop_length=hop_length, window=window_fn(n_fft), return_complex=True)\n",
    "        return stft\n",
    "\n",
    "    stft_result = perform_stft(tensor_data)\n",
    "\n",
    "    def apply_a_weighting_librosa(stft_result, sample_rate):\n",
    "        # STFT 결과로부터 FFT 크기 추정\n",
    "        n_fft = (stft_result.size(0) - 1) * 2\n",
    "        \n",
    "        # 주파수 bins 계산\n",
    "        freqs = np.linspace(0, sample_rate / 2, stft_result.size(0))\n",
    "        \n",
    "        # A-weighting dB 값 계산\n",
    "        a_weighting_db = librosa.A_weighting(freqs)\n",
    "        \n",
    "        # dB를 power scale로 변환\n",
    "        a_weighting_scale = librosa.db_to_amplitude(a_weighting_db)\n",
    "        \n",
    "        # 텐서로 변환\n",
    "        a_weighting_scale_tensor = torch.from_numpy(a_weighting_scale).to(stft_result.dtype)\n",
    "        \n",
    "        # A-weighted STFT 계산\n",
    "        a_weighted_stft = torch.abs(stft_result).log10() + a_weighting_scale_tensor.unsqueeze(1)  # 주파수 차원에 적용\n",
    "        #a_weighted_stft = torch.abs(stft_result) * a_weighting_scale_tensor.unsqueeze(1)  # 주파수 차원에 적용\n",
    "\n",
    "        return a_weighted_stft\n",
    "\n",
    "    sample_rate = 12800  # 샘플 레이트 예제 값\n",
    "    # A-weighting 적용\n",
    "    a_weighted_result = apply_a_weighting_librosa(stft_result, sample_rate)\n",
    "    _3k_result = a_weighted_result[4:129,:]\n",
    "    return torch.abs(_3k_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010e514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/librosa/core/convert.py:1870: RuntimeWarning: divide by zero encountered in log10\n",
      "  + 2 * np.log10(f_sq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([125, 349])\n",
      "torch.Size([125, 596])\n",
      "torch.Size([125, 587])\n",
      "torch.Size([125, 367])\n"
     ]
    }
   ],
   "source": [
    "es_meta_normal_list = []\n",
    "es_meta_abnormal_list = []\n",
    "\n",
    "es_abnormal_name= []\n",
    "\n",
    "for idx in es_df.index:\n",
    "    row = es_df.loc[idx]\n",
    "    filename = row['filename']\n",
    "    if row['category'] == 'Normal':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            es_meta_normal_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "    else:\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            es_meta_abnormal_list.append(ten[:,:1000].unsqueeze(0))\n",
    "            es_abnormal_name.append(filename)\n",
    "        else:\n",
    "            print(ten.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6f0d781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/librosa/core/convert.py:1870: RuntimeWarning: divide by zero encountered in log10\n",
      "  + 2 * np.log10(f_sq)\n"
     ]
    }
   ],
   "source": [
    "rps_df = pd.read_csv('1.0RPS_meta.csv')\n",
    "\n",
    "rps_normal_list = []\n",
    "rps_abnormal_list = []\n",
    "\n",
    "for idx in rps_df.index:\n",
    "    row = rps_df.loc[idx]\n",
    "    filename= row['filename']\n",
    "    if row['category'] == 'Normal':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "           rps_normal_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "    else:\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=1000:\n",
    "            rps_abnormal_list.append(ten[:,:1000].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95763883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 125, 1000])\n",
      "Output Tensor Shape: torch.Size([1, 1, 125, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__() # (125, 1000) 이라고 했을 때 \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential( # Given groups=1, weight of size [16, 128, 3, 3], expected input[1, 32, 8, 95] to have 128 channels, but got 32 channels instead\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 63, 500)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 32, 250)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 64, 16, 125)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 8, 63)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 4, 32)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 1, 2, 16)\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output shape: (b, 16, 4, 32)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2, padding=(1, 1), output_padding=(1, 0)),  # Output shape: (b, 32, 8, 63) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 64, kernel_size=3, stride=2, padding=(1,1), output_padding=(1,0)),  # Output shape: (b, 64, 16, 125) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output shape: (b, 32, 32, 250)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 16, 63, 500)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 1, 125, 1517) \n",
    "            #nn.Tanh()\n",
    "        )\n",
    "# output padding must be smaller than stride\n",
    "    def forward(self, x):\n",
    "        mid = self.encoder(x)\n",
    "        x = self.decoder(mid)\n",
    "        return x\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "test_autoencoder = Autoencoder()\n",
    "\n",
    "input_tensor = torch.randn(1, 1, 125, 1000)  # (batch_size, channels, height, width)\n",
    "\n",
    "# Forward pass through the autoencoder\n",
    "output_tensor = test_autoencoder(input_tensor)\n",
    "\n",
    "# Print the shapes of input and output tensors\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Output Tensor Shape:\", output_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9424192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 63, 500]             160\n",
      "              ReLU-2          [-1, 16, 63, 500]               0\n",
      "            Conv2d-3          [-1, 32, 32, 250]           4,640\n",
      "              ReLU-4          [-1, 32, 32, 250]               0\n",
      "            Conv2d-5          [-1, 64, 16, 125]          18,496\n",
      "              ReLU-6          [-1, 64, 16, 125]               0\n",
      "            Conv2d-7            [-1, 32, 8, 63]          18,464\n",
      "              ReLU-8            [-1, 32, 8, 63]               0\n",
      "            Conv2d-9            [-1, 16, 4, 32]           4,624\n",
      "             ReLU-10            [-1, 16, 4, 32]               0\n",
      "           Conv2d-11             [-1, 1, 2, 16]             145\n",
      "  ConvTranspose2d-12            [-1, 16, 4, 32]             160\n",
      "             ReLU-13            [-1, 16, 4, 32]               0\n",
      "  ConvTranspose2d-14            [-1, 32, 8, 63]           4,640\n",
      "             ReLU-15            [-1, 32, 8, 63]               0\n",
      "  ConvTranspose2d-16          [-1, 64, 16, 125]          18,496\n",
      "             ReLU-17          [-1, 64, 16, 125]               0\n",
      "  ConvTranspose2d-18          [-1, 32, 32, 250]          18,464\n",
      "             ReLU-19          [-1, 32, 32, 250]               0\n",
      "  ConvTranspose2d-20          [-1, 16, 63, 500]           4,624\n",
      "             ReLU-21          [-1, 16, 63, 500]               0\n",
      "  ConvTranspose2d-22         [-1, 1, 125, 1000]             145\n",
      "================================================================\n",
      "Total params: 93,058\n",
      "Trainable params: 93,058\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.48\n",
      "Forward/backward pass size (MB): 28.61\n",
      "Params size (MB): 0.35\n",
      "Estimated Total Size (MB): 29.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(test_autoencoder, (1, 125, 1000), device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "689776c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_list):\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    customdataset = CustomDataset(dataset_list)\n",
    "    train_loader = DataLoader(customdataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    autoencoder = Autoencoder().to(dev)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader): # i-th batch\n",
    "            inputs = data\n",
    "            inputs = inputs.to(dev)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'epoch : {epoch}, loss: {running_loss}')\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    encoded_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in dataset_list:\n",
    "            encoded_list.append(autoencoder.encoder(i.to(dev)).flatten())\n",
    "\n",
    "    return encoded_list\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e76a13f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss: 12.398808717727661\n",
      "epoch : 1, loss: 11.313665509223938\n",
      "epoch : 2, loss: 7.6079405546188354\n",
      "epoch : 3, loss: 3.606076329946518\n",
      "epoch : 4, loss: 2.3205993622541428\n",
      "epoch : 5, loss: 1.6610265374183655\n",
      "epoch : 6, loss: 1.350698471069336\n",
      "epoch : 7, loss: 1.1978339776396751\n",
      "epoch : 8, loss: 1.10973010212183\n",
      "epoch : 9, loss: 1.0824408382177353\n",
      "epoch : 10, loss: 1.061182513833046\n",
      "epoch : 11, loss: 1.048851229250431\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[234], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoded_es_normal_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mes_meta_normal_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[233], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     15\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# i-th batch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     18\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(dev)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoded_es_normal_list = train(es_meta_normal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dee369a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss: 11.079938650131226\n",
      "epoch : 1, loss: 10.131383061408997\n",
      "epoch : 2, loss: 7.196853876113892\n",
      "epoch : 3, loss: 4.115974992513657\n",
      "epoch : 4, loss: 2.647664353251457\n",
      "epoch : 5, loss: 1.7720158100128174\n",
      "epoch : 6, loss: 1.3735427930951118\n",
      "epoch : 7, loss: 1.1650108620524406\n",
      "epoch : 8, loss: 1.078776240348816\n",
      "epoch : 9, loss: 1.0462524518370628\n",
      "epoch : 10, loss: 1.031832866370678\n",
      "epoch : 11, loss: 1.0220938175916672\n",
      "epoch : 12, loss: 1.0143959671258926\n",
      "epoch : 13, loss: 1.0079897344112396\n",
      "epoch : 14, loss: 1.0031520798802376\n",
      "epoch : 15, loss: 0.9975587651133537\n",
      "epoch : 16, loss: 0.9931326434016228\n",
      "epoch : 17, loss: 0.9905939921736717\n",
      "epoch : 18, loss: 0.9859994798898697\n",
      "epoch : 19, loss: 0.9844277128577232\n",
      "epoch : 20, loss: 0.9813613072037697\n",
      "epoch : 21, loss: 0.9787669256329536\n",
      "epoch : 22, loss: 0.9770009964704514\n",
      "epoch : 23, loss: 0.9734100177884102\n",
      "epoch : 24, loss: 0.9718736410140991\n",
      "epoch : 25, loss: 0.9693892598152161\n",
      "epoch : 26, loss: 0.9664949178695679\n",
      "epoch : 27, loss: 0.9653673768043518\n",
      "epoch : 28, loss: 0.9621516913175583\n",
      "epoch : 29, loss: 0.9604729264974594\n",
      "epoch : 30, loss: 0.9573911428451538\n",
      "epoch : 31, loss: 0.9561891853809357\n",
      "epoch : 32, loss: 0.9533509537577629\n",
      "epoch : 33, loss: 0.9491321220993996\n",
      "epoch : 34, loss: 0.9481839835643768\n",
      "epoch : 35, loss: 0.946458712220192\n",
      "epoch : 36, loss: 0.9459522590041161\n",
      "epoch : 37, loss: 0.9433229565620422\n",
      "epoch : 38, loss: 0.9400464296340942\n",
      "epoch : 39, loss: 0.9392582178115845\n",
      "epoch : 40, loss: 0.9373940750956535\n",
      "epoch : 41, loss: 0.9352971687912941\n",
      "epoch : 42, loss: 0.9342602416872978\n",
      "epoch : 43, loss: 0.9323556274175644\n",
      "epoch : 44, loss: 0.9328925311565399\n",
      "epoch : 45, loss: 0.9326109662652016\n",
      "epoch : 46, loss: 0.9297477602958679\n",
      "epoch : 47, loss: 0.9285865724086761\n",
      "epoch : 48, loss: 0.9270656108856201\n",
      "epoch : 49, loss: 0.9257863610982895\n",
      "epoch : 50, loss: 0.9268146678805351\n",
      "epoch : 51, loss: 0.9250604957342148\n",
      "epoch : 52, loss: 0.9249439761042595\n",
      "epoch : 53, loss: 0.9249957874417305\n",
      "epoch : 54, loss: 0.9254686385393143\n",
      "epoch : 55, loss: 0.9237920641899109\n",
      "epoch : 56, loss: 0.9229114502668381\n",
      "epoch : 57, loss: 0.9214682206511497\n",
      "epoch : 58, loss: 0.9221293553709984\n",
      "epoch : 59, loss: 0.9227992594242096\n",
      "epoch : 60, loss: 0.921830803155899\n",
      "epoch : 61, loss: 0.9216693267226219\n",
      "epoch : 62, loss: 0.919551432132721\n",
      "epoch : 63, loss: 0.9187274128198624\n",
      "epoch : 64, loss: 0.919168658554554\n",
      "epoch : 65, loss: 0.9186002165079117\n",
      "epoch : 66, loss: 0.9187401905655861\n",
      "epoch : 67, loss: 0.9189632833003998\n",
      "epoch : 68, loss: 0.9176901802420616\n",
      "epoch : 69, loss: 0.9186087474226952\n",
      "epoch : 70, loss: 0.9180984124541283\n",
      "epoch : 71, loss: 0.9186220541596413\n",
      "epoch : 72, loss: 0.9197287559509277\n",
      "epoch : 73, loss: 0.9200107082724571\n",
      "epoch : 74, loss: 0.9175349622964859\n",
      "epoch : 75, loss: 0.9170094281435013\n",
      "epoch : 76, loss: 0.9159093201160431\n",
      "epoch : 77, loss: 0.915660098195076\n",
      "epoch : 78, loss: 0.9156109467148781\n",
      "epoch : 79, loss: 0.9161501824855804\n",
      "epoch : 80, loss: 0.9153948798775673\n",
      "epoch : 81, loss: 0.9155461713671684\n",
      "epoch : 82, loss: 0.9171703979372978\n",
      "epoch : 83, loss: 0.9175311326980591\n",
      "epoch : 84, loss: 0.9162263199687004\n",
      "epoch : 85, loss: 0.915410079061985\n",
      "epoch : 86, loss: 0.9132278636097908\n",
      "epoch : 87, loss: 0.9144013524055481\n",
      "epoch : 88, loss: 0.9143797904253006\n",
      "epoch : 89, loss: 0.9135773032903671\n",
      "epoch : 90, loss: 0.9124540910124779\n",
      "epoch : 91, loss: 0.9134592637419701\n",
      "epoch : 92, loss: 0.912827380001545\n",
      "epoch : 93, loss: 0.914495475590229\n",
      "epoch : 94, loss: 0.913325697183609\n",
      "epoch : 95, loss: 0.915193498134613\n",
      "epoch : 96, loss: 0.9126422330737114\n",
      "epoch : 97, loss: 0.9125241562724113\n",
      "epoch : 98, loss: 0.9135582819581032\n",
      "epoch : 99, loss: 0.9117186889052391\n",
      "Finished Training\n",
      "epoch : 0, loss: 3.3196639716625214\n",
      "epoch : 1, loss: 3.0636522471904755\n",
      "epoch : 2, loss: 2.688199907541275\n",
      "epoch : 3, loss: 2.289773613214493\n",
      "epoch : 4, loss: 1.606705516576767\n",
      "epoch : 5, loss: 1.128588080406189\n",
      "epoch : 6, loss: 0.9638879597187042\n",
      "epoch : 7, loss: 0.8980376347899437\n",
      "epoch : 8, loss: 0.866058699786663\n",
      "epoch : 9, loss: 0.8500263839960098\n",
      "epoch : 10, loss: 0.8388262465596199\n",
      "epoch : 11, loss: 0.8304501846432686\n",
      "epoch : 12, loss: 0.8240520060062408\n",
      "epoch : 13, loss: 0.8181138783693314\n",
      "epoch : 14, loss: 0.8128334656357765\n",
      "epoch : 15, loss: 0.80668955296278\n",
      "epoch : 16, loss: 0.802318811416626\n",
      "epoch : 17, loss: 0.7973776534199715\n",
      "epoch : 18, loss: 0.7913440614938736\n",
      "epoch : 19, loss: 0.78709726780653\n",
      "epoch : 20, loss: 0.7835394367575645\n",
      "epoch : 21, loss: 0.7780182361602783\n",
      "epoch : 22, loss: 0.7730308100581169\n",
      "epoch : 23, loss: 0.7695210427045822\n",
      "epoch : 24, loss: 0.7671209797263145\n",
      "epoch : 25, loss: 0.7639968693256378\n",
      "epoch : 26, loss: 0.7617933452129364\n",
      "epoch : 27, loss: 0.757853701710701\n",
      "epoch : 28, loss: 0.7570455372333527\n",
      "epoch : 29, loss: 0.7563565969467163\n",
      "epoch : 30, loss: 0.7546718940138817\n",
      "epoch : 31, loss: 0.7535055726766586\n",
      "epoch : 32, loss: 0.7503697648644447\n",
      "epoch : 33, loss: 0.7499844506382942\n",
      "epoch : 34, loss: 0.7471761107444763\n",
      "epoch : 35, loss: 0.7458241283893585\n",
      "epoch : 36, loss: 0.7459010034799576\n",
      "epoch : 37, loss: 0.7453008443117142\n",
      "epoch : 38, loss: 0.7421856969594955\n",
      "epoch : 39, loss: 0.7413579523563385\n",
      "epoch : 40, loss: 0.7401089519262314\n",
      "epoch : 41, loss: 0.7401601895689964\n",
      "epoch : 42, loss: 0.7409054264426231\n",
      "epoch : 43, loss: 0.737775094807148\n",
      "epoch : 44, loss: 0.736452266573906\n",
      "epoch : 45, loss: 0.7357332929968834\n",
      "epoch : 46, loss: 0.7366973087191582\n",
      "epoch : 47, loss: 0.7345371097326279\n",
      "epoch : 48, loss: 0.7337636277079582\n",
      "epoch : 49, loss: 0.7329853624105453\n",
      "epoch : 50, loss: 0.7337894812226295\n",
      "epoch : 51, loss: 0.7316104844212532\n",
      "epoch : 52, loss: 0.7318845689296722\n",
      "epoch : 53, loss: 0.7312038838863373\n",
      "epoch : 54, loss: 0.7315040677785873\n",
      "epoch : 55, loss: 0.7304848432540894\n",
      "epoch : 56, loss: 0.7312796860933304\n",
      "epoch : 57, loss: 0.730939194560051\n",
      "epoch : 58, loss: 0.7283158004283905\n",
      "epoch : 59, loss: 0.728727400302887\n",
      "epoch : 60, loss: 0.7282459288835526\n",
      "epoch : 61, loss: 0.7285254672169685\n",
      "epoch : 62, loss: 0.7283967509865761\n",
      "epoch : 63, loss: 0.7279379293322563\n",
      "epoch : 64, loss: 0.7266788631677628\n",
      "epoch : 65, loss: 0.7258169427514076\n",
      "epoch : 66, loss: 0.7259962558746338\n",
      "epoch : 67, loss: 0.7247289791703224\n",
      "epoch : 68, loss: 0.7249368950724602\n",
      "epoch : 69, loss: 0.725619412958622\n",
      "epoch : 70, loss: 0.7243334352970123\n",
      "epoch : 71, loss: 0.7251574993133545\n",
      "epoch : 72, loss: 0.7238898426294327\n",
      "epoch : 73, loss: 0.7240538373589516\n",
      "epoch : 74, loss: 0.7234374806284904\n",
      "epoch : 75, loss: 0.7230778858065605\n",
      "epoch : 76, loss: 0.7223038151860237\n",
      "epoch : 77, loss: 0.7232435792684555\n",
      "epoch : 78, loss: 0.7228711545467377\n",
      "epoch : 79, loss: 0.7221237421035767\n",
      "epoch : 80, loss: 0.7219525948166847\n",
      "epoch : 81, loss: 0.7241622284054756\n",
      "epoch : 82, loss: 0.7239506840705872\n",
      "epoch : 83, loss: 0.7210965305566788\n",
      "epoch : 84, loss: 0.7219533696770668\n",
      "epoch : 85, loss: 0.7224612683057785\n",
      "epoch : 86, loss: 0.7218047007918358\n",
      "epoch : 87, loss: 0.7230024337768555\n",
      "epoch : 88, loss: 0.7211854681372643\n",
      "epoch : 89, loss: 0.7199115455150604\n",
      "epoch : 90, loss: 0.7206401079893112\n",
      "epoch : 91, loss: 0.7236683890223503\n",
      "epoch : 92, loss: 0.7194677069783211\n",
      "epoch : 93, loss: 0.7185283005237579\n",
      "epoch : 94, loss: 0.7192808166146278\n",
      "epoch : 95, loss: 0.7187171950936317\n",
      "epoch : 96, loss: 0.7193244621157646\n",
      "epoch : 97, loss: 0.7190281897783279\n",
      "epoch : 98, loss: 0.71852046251297\n",
      "epoch : 99, loss: 0.7192145437002182\n",
      "Finished Training\n",
      "epoch : 0, loss: 3.450452208518982\n",
      "epoch : 1, loss: 3.270792841911316\n",
      "epoch : 2, loss: 3.0899808406829834\n",
      "epoch : 3, loss: 2.8237751126289368\n",
      "epoch : 4, loss: 2.468116879463196\n",
      "epoch : 5, loss: 1.738846331834793\n",
      "epoch : 6, loss: 1.3665635734796524\n",
      "epoch : 7, loss: 1.1828850656747818\n",
      "epoch : 8, loss: 0.9896797686815262\n",
      "epoch : 9, loss: 0.8975795805454254\n",
      "epoch : 10, loss: 0.7998746782541275\n",
      "epoch : 11, loss: 0.7293591201305389\n",
      "epoch : 12, loss: 0.6549639701843262\n",
      "epoch : 13, loss: 0.5953977257013321\n",
      "epoch : 14, loss: 0.5564202070236206\n",
      "epoch : 15, loss: 0.5372996926307678\n",
      "epoch : 16, loss: 0.5279015377163887\n",
      "epoch : 17, loss: 0.5171955525875092\n",
      "epoch : 18, loss: 0.514942117035389\n",
      "epoch : 19, loss: 0.5048528462648392\n",
      "epoch : 20, loss: 0.5013996362686157\n",
      "epoch : 21, loss: 0.4979507774114609\n",
      "epoch : 22, loss: 0.4964817613363266\n",
      "epoch : 23, loss: 0.4926292449235916\n",
      "epoch : 24, loss: 0.4899357780814171\n",
      "epoch : 25, loss: 0.49102238565683365\n",
      "epoch : 26, loss: 0.48829592764377594\n",
      "epoch : 27, loss: 0.48472607880830765\n",
      "epoch : 28, loss: 0.4832165166735649\n",
      "epoch : 29, loss: 0.4824278950691223\n",
      "epoch : 30, loss: 0.4809422269463539\n",
      "epoch : 31, loss: 0.4794284626841545\n",
      "epoch : 32, loss: 0.4787501320242882\n",
      "epoch : 33, loss: 0.4785655811429024\n",
      "epoch : 34, loss: 0.4752475991845131\n",
      "epoch : 35, loss: 0.47610069066286087\n",
      "epoch : 36, loss: 0.4745573177933693\n",
      "epoch : 37, loss: 0.47383085638284683\n",
      "epoch : 38, loss: 0.4733499065041542\n",
      "epoch : 39, loss: 0.4737274944782257\n",
      "epoch : 40, loss: 0.4748391881585121\n",
      "epoch : 41, loss: 0.4743766710162163\n",
      "epoch : 42, loss: 0.4713907763361931\n",
      "epoch : 43, loss: 0.47121789306402206\n",
      "epoch : 44, loss: 0.471026174724102\n",
      "epoch : 45, loss: 0.47229964286088943\n",
      "epoch : 46, loss: 0.47126249969005585\n",
      "epoch : 47, loss: 0.4711810499429703\n",
      "epoch : 48, loss: 0.467667393386364\n",
      "epoch : 49, loss: 0.4683399498462677\n",
      "epoch : 50, loss: 0.46743539720773697\n",
      "epoch : 51, loss: 0.4672631472349167\n",
      "epoch : 52, loss: 0.46912846714258194\n",
      "epoch : 53, loss: 0.47005368024110794\n",
      "epoch : 54, loss: 0.46670009195804596\n",
      "epoch : 55, loss: 0.4644826725125313\n",
      "epoch : 56, loss: 0.4646349474787712\n",
      "epoch : 57, loss: 0.46347228437662125\n",
      "epoch : 58, loss: 0.4630540609359741\n",
      "epoch : 59, loss: 0.4627429470419884\n",
      "epoch : 60, loss: 0.4626396670937538\n",
      "epoch : 61, loss: 0.4619777426123619\n",
      "epoch : 62, loss: 0.46069231629371643\n",
      "epoch : 63, loss: 0.4612279310822487\n",
      "epoch : 64, loss: 0.4593259245157242\n",
      "epoch : 65, loss: 0.4591480568051338\n",
      "epoch : 66, loss: 0.45990584790706635\n",
      "epoch : 67, loss: 0.4574199244379997\n",
      "epoch : 68, loss: 0.45641660690307617\n",
      "epoch : 69, loss: 0.4573994129896164\n",
      "epoch : 70, loss: 0.45730769634246826\n",
      "epoch : 71, loss: 0.4571073353290558\n",
      "epoch : 72, loss: 0.455811470746994\n",
      "epoch : 73, loss: 0.45688675343990326\n",
      "epoch : 74, loss: 0.456209659576416\n",
      "epoch : 75, loss: 0.4564841091632843\n",
      "epoch : 76, loss: 0.4555792883038521\n",
      "epoch : 77, loss: 0.4552663266658783\n",
      "epoch : 78, loss: 0.45452792942523956\n",
      "epoch : 79, loss: 0.4532638192176819\n",
      "epoch : 80, loss: 0.45317113399505615\n",
      "epoch : 81, loss: 0.45218853652477264\n",
      "epoch : 82, loss: 0.4524286910891533\n",
      "epoch : 83, loss: 0.4533531963825226\n",
      "epoch : 84, loss: 0.4520570933818817\n",
      "epoch : 85, loss: 0.45187272876501083\n",
      "epoch : 86, loss: 0.45187921077013016\n",
      "epoch : 87, loss: 0.4511106535792351\n",
      "epoch : 88, loss: 0.45065711438655853\n",
      "epoch : 89, loss: 0.45088406652212143\n",
      "epoch : 90, loss: 0.4508257582783699\n",
      "epoch : 91, loss: 0.4509179890155792\n",
      "epoch : 92, loss: 0.450225405395031\n",
      "epoch : 93, loss: 0.4495336264371872\n",
      "epoch : 94, loss: 0.44918107241392136\n",
      "epoch : 95, loss: 0.44866080582141876\n",
      "epoch : 96, loss: 0.4491206854581833\n",
      "epoch : 97, loss: 0.4480423927307129\n",
      "epoch : 98, loss: 0.4481333792209625\n",
      "epoch : 99, loss: 0.44916070997714996\n",
      "Finished Training\n",
      "epoch : 0, loss: 3.353152096271515\n",
      "epoch : 1, loss: 3.319234013557434\n",
      "epoch : 2, loss: 3.112251043319702\n",
      "epoch : 3, loss: 3.1154388189315796\n",
      "epoch : 4, loss: 2.9126018285751343\n",
      "epoch : 5, loss: 2.4491446018218994\n",
      "epoch : 6, loss: 1.7077146470546722\n",
      "epoch : 7, loss: 1.239484429359436\n",
      "epoch : 8, loss: 0.9862272590398788\n",
      "epoch : 9, loss: 0.7905167192220688\n",
      "epoch : 10, loss: 0.6550735533237457\n",
      "epoch : 11, loss: 0.5712767541408539\n",
      "epoch : 12, loss: 0.5161466896533966\n",
      "epoch : 13, loss: 0.48527414351701736\n",
      "epoch : 14, loss: 0.4674714207649231\n",
      "epoch : 15, loss: 0.437452107667923\n",
      "epoch : 16, loss: 0.4295371249318123\n",
      "epoch : 17, loss: 0.42650006711483\n",
      "epoch : 18, loss: 0.40831732004880905\n",
      "epoch : 19, loss: 0.404249869287014\n",
      "epoch : 20, loss: 0.4006439745426178\n",
      "epoch : 21, loss: 0.4016275182366371\n",
      "epoch : 22, loss: 0.39621974527835846\n",
      "epoch : 23, loss: 0.39616817981004715\n",
      "epoch : 24, loss: 0.39033491909503937\n",
      "epoch : 25, loss: 0.38865208625793457\n",
      "epoch : 26, loss: 0.38825349509716034\n",
      "epoch : 27, loss: 0.3852618932723999\n",
      "epoch : 28, loss: 0.3838733583688736\n",
      "epoch : 29, loss: 0.38813839852809906\n",
      "epoch : 30, loss: 0.38398973643779755\n",
      "epoch : 31, loss: 0.38197600841522217\n",
      "epoch : 32, loss: 0.38137055933475494\n",
      "epoch : 33, loss: 0.37755604088306427\n",
      "epoch : 34, loss: 0.37818794697523117\n",
      "epoch : 35, loss: 0.3802572265267372\n",
      "epoch : 36, loss: 0.3740076869726181\n",
      "epoch : 37, loss: 0.3768920302391052\n",
      "epoch : 38, loss: 0.3783115968108177\n",
      "epoch : 39, loss: 0.37459689378738403\n",
      "epoch : 40, loss: 0.3703381419181824\n",
      "epoch : 41, loss: 0.37557119131088257\n",
      "epoch : 42, loss: 0.3728014901280403\n",
      "epoch : 43, loss: 0.3701842501759529\n",
      "epoch : 44, loss: 0.36763064563274384\n",
      "epoch : 45, loss: 0.36640968173742294\n",
      "epoch : 46, loss: 0.36671601980924606\n",
      "epoch : 47, loss: 0.3626477047801018\n",
      "epoch : 48, loss: 0.3672536611557007\n",
      "epoch : 49, loss: 0.36235444992780685\n",
      "epoch : 50, loss: 0.3712104633450508\n",
      "epoch : 51, loss: 0.36075928062200546\n",
      "epoch : 52, loss: 0.36496949940919876\n",
      "epoch : 53, loss: 0.36528314650058746\n",
      "epoch : 54, loss: 0.36793597787618637\n",
      "epoch : 55, loss: 0.3608119487762451\n",
      "epoch : 56, loss: 0.3614910766482353\n",
      "epoch : 57, loss: 0.36367127299308777\n",
      "epoch : 58, loss: 0.36315276473760605\n",
      "epoch : 59, loss: 0.36217349022626877\n",
      "epoch : 60, loss: 0.36199264228343964\n",
      "epoch : 61, loss: 0.35961825400590897\n",
      "epoch : 62, loss: 0.35978537052869797\n",
      "epoch : 63, loss: 0.3586878180503845\n",
      "epoch : 64, loss: 0.35998136550188065\n",
      "epoch : 65, loss: 0.36312783509492874\n",
      "epoch : 66, loss: 0.35902947932481766\n",
      "epoch : 67, loss: 0.35790732502937317\n",
      "epoch : 68, loss: 0.3576902002096176\n",
      "epoch : 69, loss: 0.3511480912566185\n",
      "epoch : 70, loss: 0.35658513754606247\n",
      "epoch : 71, loss: 0.3579590916633606\n",
      "epoch : 72, loss: 0.3577253371477127\n",
      "epoch : 73, loss: 0.3558935523033142\n",
      "epoch : 74, loss: 0.35942768305540085\n",
      "epoch : 75, loss: 0.35844700783491135\n",
      "epoch : 76, loss: 0.35306283831596375\n",
      "epoch : 77, loss: 0.3600456789135933\n",
      "epoch : 78, loss: 0.35515958070755005\n",
      "epoch : 79, loss: 0.36024831235408783\n",
      "epoch : 80, loss: 0.3552629351615906\n",
      "epoch : 81, loss: 0.35399995744228363\n",
      "epoch : 82, loss: 0.35911544412374496\n",
      "epoch : 83, loss: 0.35489489138126373\n",
      "epoch : 84, loss: 0.35424429178237915\n",
      "epoch : 85, loss: 0.3553558364510536\n",
      "epoch : 86, loss: 0.35796065628528595\n",
      "epoch : 87, loss: 0.3538641780614853\n",
      "epoch : 88, loss: 0.35410384088754654\n",
      "epoch : 89, loss: 0.34897275269031525\n",
      "epoch : 90, loss: 0.3513603210449219\n",
      "epoch : 91, loss: 0.35168737918138504\n",
      "epoch : 92, loss: 0.35106441378593445\n",
      "epoch : 93, loss: 0.3489276245236397\n",
      "epoch : 94, loss: 0.35124780237674713\n",
      "epoch : 95, loss: 0.34742313623428345\n",
      "epoch : 96, loss: 0.35151153057813644\n",
      "epoch : 97, loss: 0.3430381119251251\n",
      "epoch : 98, loss: 0.3544583320617676\n",
      "epoch : 99, loss: 0.3463219478726387\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "encoded_es_normal_list = train(es_meta_normal_list)\n",
    "encoded_es_abnormal_list = train(es_meta_abnormal_list)\n",
    "# encoded_line_normal_list = train(line_meta_normal_list)\n",
    "# encoded_line_abnormal_list = train(line_meta_abnormal_list)\n",
    "encoded_rps_normal_list = train(rps_normal_list)\n",
    "encoded_rps_abnormal_list = train(rps_abnormal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6b5ece0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "243\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_es_normal_list))\n",
    "print(len(encoded_es_abnormal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9e48b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-25.0180, -20.0861, -20.0657, -20.3882, -19.5037, -18.2113, -18.6844,\n",
      "        -17.7336, -17.3145, -16.8534, -16.1940, -14.3780, -14.2186, -13.2293,\n",
      "        -12.3793, -11.3426, -34.0599, -33.6596, -29.7357, -26.8967, -25.9233,\n",
      "        -22.5421, -20.4267, -18.6510, -17.0051, -15.6653, -14.7202, -13.6317,\n",
      "        -12.9097, -11.6508, -10.8183, -10.1089], device='cuda:0')\n",
      "tensor([-45.1905, -44.3485, -44.1869, -37.8495, -36.1261, -33.4207, -31.1672,\n",
      "        -33.2614, -30.1946, -30.0322, -27.9996, -27.2515, -23.4164, -16.5871,\n",
      "        -15.1389, -16.9202, -69.3491, -64.7952, -58.7570, -46.1870, -34.1557,\n",
      "        -25.0276, -18.7406, -14.1415, -12.2227, -10.3272, -12.7524,  -7.1964,\n",
      "          1.7215,  12.8773,  16.1953,  12.0615], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "idx = 13\n",
    "print(encoded_es_normal_list[idx])\n",
    "print(encoded_es_abnormal_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0468d08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_rps_normal_list))\n",
    "print(len(encoded_rps_abnormal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f1083e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157,\n",
      "        0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157,\n",
      "        0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157, 0.7157,\n",
      "        0.7157, 0.7157, 0.7157, 0.7157, 0.7157], device='cuda:0')\n",
      "tensor([22.0828, 29.9187, 28.6448, 29.7196, 29.2427, 29.5657, 29.6560, 29.6708,\n",
      "        29.3107, 29.6903, 28.8966, 29.5175, 28.5672, 29.0806, 29.6665, 23.6228,\n",
      "        15.2068, 23.2690, 23.2540, 23.1654, 22.2447, 23.2502, 23.8726, 22.6562,\n",
      "        22.7802, 22.3268, 23.6160, 22.3855, 23.0119, 19.4682, 18.8828, 23.2763],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "idx = 32\n",
    "print(encoded_rps_normal_list[idx])\n",
    "print(encoded_rps_abnormal_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b75927e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "es_concated = encoded_es_normal_list + encoded_es_abnormal_list\n",
    "es_concated = torch.stack(es_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "es_cluster_labels = kmeans.fit_predict(es_concated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9b84ccbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5031c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n",
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n",
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n",
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n",
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n",
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n",
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n",
      "./data/wav_문제품DATAa/#20_SX2_C_ES_RH_2/#20_SX2_C_ES_RH_2_3_Mic_30cm_S.wav\n"
     ]
    }
   ],
   "source": [
    "index = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
    "\n",
    "for i in index:\n",
    "    if i == 0:\n",
    "        print(es_abnormal_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852c2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "3aa87fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_normal_cnt={0:0, 1:0} # total should be: 298\n",
    "es_abnormal_cnt={0:0, 1:0} # 243\n",
    "for i, cl_label in enumerate(es_cluster_labels):\n",
    "    if i < len(encoded_es_normal_list):\n",
    "        es_normal_cnt[cl_label]+= 1\n",
    "    else:\n",
    "        es_abnormal_cnt[cl_label] += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9bdc0405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 298, 1: 0}\n",
      "{0: 8, 1: 235}\n"
     ]
    }
   ],
   "source": [
    "print(es_normal_cnt)\n",
    "print(es_abnormal_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "968df44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 145, 1: 0}\n",
      "{0: 0, 1: 102}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "rps_concated = encoded_rps_normal_list + encoded_rps_abnormal_list\n",
    "rps_concated = torch.stack(rps_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "rps_cluster_labels = kmeans.fit_predict(rps_concated)\n",
    "\n",
    "rps_normal_cnt={0:0, 1:0} # total should be: 145\n",
    "rps_abnormal_cnt={0:0, 1:0} # 102\n",
    "\n",
    "for i, cl_label in enumerate(rps_cluster_labels):\n",
    "    if i < len(encoded_rps_normal_list):\n",
    "        rps_normal_cnt[cl_label]+= 1\n",
    "    else:\n",
    "        rps_abnormal_cnt[cl_label] += 1\n",
    "    \n",
    "print(rps_normal_cnt)\n",
    "print(rps_abnormal_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64163d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eede07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7d1a26cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/librosa/core/convert.py:1870: RuntimeWarning: divide by zero encountered in log10\n",
      "  + 2 * np.log10(f_sq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([125, 367])\n",
      "torch.Size([125, 459])\n",
      "torch.Size([125, 437])\n",
      "torch.Size([125, 404])\n",
      "torch.Size([125, 403])\n",
      "torch.Size([125, 461])\n",
      "torch.Size([125, 447])\n",
      "torch.Size([125, 397])\n",
      "torch.Size([125, 403])\n",
      "torch.Size([125, 362])\n",
      "torch.Size([125, 362])\n",
      "torch.Size([125, 363])\n",
      "torch.Size([125, 461])\n",
      "torch.Size([125, 493])\n",
      "torch.Size([125, 372])\n",
      "torch.Size([125, 456])\n",
      "torch.Size([125, 481])\n",
      "torch.Size([125, 455])\n",
      "torch.Size([125, 376])\n",
      "torch.Size([125, 372])\n",
      "torch.Size([125, 442])\n",
      "torch.Size([125, 358])\n",
      "torch.Size([125, 361])\n",
      "torch.Size([125, 357])\n",
      "torch.Size([125, 448])\n",
      "torch.Size([125, 360])\n",
      "torch.Size([125, 362])\n",
      "torch.Size([125, 360])\n",
      "torch.Size([125, 357])\n",
      "torch.Size([125, 357])\n",
      "torch.Size([125, 359])\n",
      "torch.Size([125, 443])\n",
      "torch.Size([125, 439])\n",
      "torch.Size([125, 452])\n",
      "torch.Size([125, 449])\n",
      "torch.Size([125, 452])\n",
      "torch.Size([125, 454])\n",
      "torch.Size([125, 450])\n",
      "torch.Size([125, 464])\n",
      "torch.Size([125, 485])\n",
      "torch.Size([125, 493])\n",
      "torch.Size([125, 451])\n",
      "torch.Size([125, 480])\n",
      "torch.Size([125, 466])\n",
      "torch.Size([125, 455])\n",
      "torch.Size([125, 477])\n",
      "torch.Size([125, 456])\n",
      "torch.Size([125, 495])\n",
      "torch.Size([125, 480])\n",
      "torch.Size([125, 448])\n",
      "torch.Size([125, 497])\n",
      "torch.Size([125, 369])\n",
      "torch.Size([125, 427])\n",
      "torch.Size([125, 445])\n",
      "torch.Size([125, 439])\n",
      "torch.Size([125, 447])\n",
      "torch.Size([125, 451])\n",
      "torch.Size([125, 323])\n",
      "torch.Size([125, 455])\n",
      "torch.Size([125, 481])\n",
      "torch.Size([125, 496])\n",
      "torch.Size([125, 475])\n",
      "torch.Size([125, 482])\n",
      "torch.Size([125, 490])\n",
      "torch.Size([125, 465])\n",
      "torch.Size([125, 358])\n",
      "torch.Size([125, 469])\n",
      "torch.Size([125, 475])\n",
      "torch.Size([125, 470])\n",
      "torch.Size([125, 349])\n",
      "torch.Size([125, 367])\n",
      "torch.Size([125, 491])\n",
      "torch.Size([125, 468])\n",
      "torch.Size([125, 452])\n",
      "torch.Size([125, 488])\n",
      "torch.Size([125, 480])\n",
      "torch.Size([125, 447])\n",
      "torch.Size([125, 433])\n",
      "torch.Size([125, 478])\n",
      "torch.Size([125, 485])\n",
      "torch.Size([125, 478])\n",
      "torch.Size([125, 434])\n",
      "torch.Size([125, 489])\n",
      "torch.Size([125, 496])\n",
      "torch.Size([125, 352])\n",
      "torch.Size([125, 448])\n",
      "torch.Size([125, 355])\n",
      "torch.Size([125, 444])\n",
      "torch.Size([125, 443])\n",
      "torch.Size([125, 437])\n",
      "torch.Size([125, 452])\n",
      "torch.Size([125, 493])\n",
      "torch.Size([125, 436])\n",
      "torch.Size([125, 486])\n",
      "torch.Size([125, 357])\n",
      "torch.Size([125, 491])\n",
      "torch.Size([125, 468])\n",
      "torch.Size([125, 479])\n",
      "torch.Size([125, 368])\n",
      "torch.Size([125, 444])\n",
      "torch.Size([125, 444])\n",
      "torch.Size([125, 495])\n",
      "torch.Size([125, 438])\n"
     ]
    }
   ],
   "source": [
    "line_df = pd.read_csv('LINE_meta.csv')\n",
    "\n",
    "line_meta_normal_list = []\n",
    "line_meta_abnormal_list = []\n",
    "\n",
    "for idx in line_df.index:\n",
    "    row = line_df.loc[idx]\n",
    "    filename= row['filename']\n",
    "    if row['category'] == 'Normal':\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=500:\n",
    "            line_meta_normal_list.append(ten[:,:500].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n",
    "    else:\n",
    "        ten = get_tensor(filename)\n",
    "        if ten.shape[1]>=500:\n",
    "            line_meta_abnormal_list.append(ten[:,:500].unsqueeze(0))\n",
    "        else:\n",
    "            print(ten.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e4c1a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "print(len(line_meta_normal_list))\n",
    "print(len(line_meta_abnormal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "55788577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class line_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(line_Autoencoder, self).__init__() # (125, 1000) 이라고 했을 때 \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential( # Given groups=1, weight of size [16, 128, 3, 3], expected input[1, 32, 8, 95] to have 128 channels, but got 32 channels instead\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 63, 250)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 32, 125)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 64, 16, 63)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 32, 8, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=2, padding=1),  # Output shape: (b, 16, 4, 16)\n",
    "            \n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output shape: (b, 16, 8, 32)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2, padding=(1, 1), output_padding=(1,0)),  # Output shape: (b, 32, 16, 63) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=(1,1), output_padding=(1,0)),  # Output shape: (b, 64, 32, 125) \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 32, 63, 250)  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,1)),  # Output shape: (b, 16, 125, 500)  \n",
    "            \n",
    "        )\n",
    "# output padding must be smaller than stride\n",
    "    def forward(self, x):\n",
    "        mid = self.encoder(x)\n",
    "        x = self.decoder(mid)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "59fcba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_train(dataset_list):\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    customdataset = CustomDataset(dataset_list)\n",
    "    train_loader = DataLoader(customdataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    autoencoder = line_Autoencoder().to(dev)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader): # i-th batch\n",
    "            inputs = data\n",
    "            inputs = inputs.to(dev)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'epoch : {epoch}, loss: {running_loss}')\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    encoded_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in dataset_list:\n",
    "            encoded_list.append(autoencoder.encoder(i.to(dev)).flatten())\n",
    "\n",
    "    return encoded_list\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f58a2ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss: 18.79534387588501\n",
      "epoch : 1, loss: 18.005507469177246\n",
      "epoch : 2, loss: 16.34283137321472\n",
      "epoch : 3, loss: 8.980451703071594\n",
      "epoch : 4, loss: 4.571180433034897\n",
      "epoch : 5, loss: 3.1808572113513947\n",
      "epoch : 6, loss: 2.4019822478294373\n",
      "epoch : 7, loss: 1.9643169939517975\n",
      "epoch : 8, loss: 1.6765075474977493\n",
      "epoch : 9, loss: 1.491836428642273\n",
      "epoch : 10, loss: 1.3597840815782547\n",
      "epoch : 11, loss: 1.255462035536766\n",
      "epoch : 12, loss: 1.1719489991664886\n",
      "epoch : 13, loss: 1.1225342750549316\n",
      "epoch : 14, loss: 1.0924255549907684\n",
      "epoch : 15, loss: 1.0715667307376862\n",
      "epoch : 16, loss: 1.0568200051784515\n",
      "epoch : 17, loss: 1.0416569113731384\n",
      "epoch : 18, loss: 1.031990334391594\n",
      "epoch : 19, loss: 1.020093396306038\n",
      "epoch : 20, loss: 1.008617989718914\n",
      "epoch : 21, loss: 0.9991780444979668\n",
      "epoch : 22, loss: 0.99294213950634\n",
      "epoch : 23, loss: 0.9882945641875267\n",
      "epoch : 24, loss: 0.9846357852220535\n",
      "epoch : 25, loss: 0.9796131327748299\n",
      "epoch : 26, loss: 0.9754904210567474\n",
      "epoch : 27, loss: 0.9704821705818176\n",
      "epoch : 28, loss: 0.9668780416250229\n",
      "epoch : 29, loss: 0.9637914374470711\n",
      "epoch : 30, loss: 0.9603540524840355\n",
      "epoch : 31, loss: 0.9564294591546059\n",
      "epoch : 32, loss: 0.952228769659996\n",
      "epoch : 33, loss: 0.9482240825891495\n",
      "epoch : 34, loss: 0.9444745555520058\n",
      "epoch : 35, loss: 0.9429021924734116\n",
      "epoch : 36, loss: 0.9383292496204376\n",
      "epoch : 37, loss: 0.933597519993782\n",
      "epoch : 38, loss: 0.9304173439741135\n",
      "epoch : 39, loss: 0.9280890822410583\n",
      "epoch : 40, loss: 0.9249287620186806\n",
      "epoch : 41, loss: 0.9184969291090965\n",
      "epoch : 42, loss: 0.9143994674086571\n",
      "epoch : 43, loss: 0.912293516099453\n",
      "epoch : 44, loss: 0.9084572419524193\n",
      "epoch : 45, loss: 0.9041455686092377\n",
      "epoch : 46, loss: 0.8983578979969025\n",
      "epoch : 47, loss: 0.8949676603078842\n",
      "epoch : 48, loss: 0.8924447149038315\n",
      "epoch : 49, loss: 0.8869880810379982\n",
      "epoch : 50, loss: 0.8829610422253609\n",
      "epoch : 51, loss: 0.8815869316458702\n",
      "epoch : 52, loss: 0.8738455176353455\n",
      "epoch : 53, loss: 0.8691473677754402\n",
      "epoch : 54, loss: 0.867090567946434\n",
      "epoch : 55, loss: 0.8626244962215424\n",
      "epoch : 56, loss: 0.8595985472202301\n",
      "epoch : 57, loss: 0.8553280830383301\n",
      "epoch : 58, loss: 0.8528998345136642\n",
      "epoch : 59, loss: 0.8498198539018631\n",
      "epoch : 60, loss: 0.8472369387745857\n",
      "epoch : 61, loss: 0.8447313383221626\n",
      "epoch : 62, loss: 0.8405884653329849\n",
      "epoch : 63, loss: 0.8398903459310532\n",
      "epoch : 64, loss: 0.8352050930261612\n",
      "epoch : 65, loss: 0.8326834589242935\n",
      "epoch : 66, loss: 0.8334214761853218\n",
      "epoch : 67, loss: 0.8308596834540367\n",
      "epoch : 68, loss: 0.8280548006296158\n",
      "epoch : 69, loss: 0.8274914994835854\n",
      "epoch : 70, loss: 0.825405165553093\n",
      "epoch : 71, loss: 0.823216512799263\n",
      "epoch : 72, loss: 0.822398878633976\n",
      "epoch : 73, loss: 0.8203467130661011\n",
      "epoch : 74, loss: 0.8200378939509392\n",
      "epoch : 75, loss: 0.8206089735031128\n",
      "epoch : 76, loss: 0.8171914294362068\n",
      "epoch : 77, loss: 0.8154777511954308\n",
      "epoch : 78, loss: 0.8144156560301781\n",
      "epoch : 79, loss: 0.8127208799123764\n",
      "epoch : 80, loss: 0.8123979941010475\n",
      "epoch : 81, loss: 0.8122247904539108\n",
      "epoch : 82, loss: 0.8092613592743874\n",
      "epoch : 83, loss: 0.8111122027039528\n",
      "epoch : 84, loss: 0.811730220913887\n",
      "epoch : 85, loss: 0.81300388276577\n",
      "epoch : 86, loss: 0.8065223693847656\n",
      "epoch : 87, loss: 0.8064688220620155\n",
      "epoch : 88, loss: 0.8060113415122032\n",
      "epoch : 89, loss: 0.8040918037295341\n",
      "epoch : 90, loss: 0.8047995194792747\n",
      "epoch : 91, loss: 0.8035255894064903\n",
      "epoch : 92, loss: 0.8051760867238045\n",
      "epoch : 93, loss: 0.8009069114923477\n",
      "epoch : 94, loss: 0.8005362376570702\n",
      "epoch : 95, loss: 0.8010497316718102\n",
      "epoch : 96, loss: 0.7986820638179779\n",
      "epoch : 97, loss: 0.7982861027121544\n",
      "epoch : 98, loss: 0.799313448369503\n",
      "epoch : 99, loss: 0.8045290634036064\n",
      "Finished Training\n",
      "epoch : 0, loss: 15.729532837867737\n",
      "epoch : 1, loss: 15.675914525985718\n",
      "epoch : 2, loss: 13.69747269153595\n",
      "epoch : 3, loss: 7.851004600524902\n",
      "epoch : 4, loss: 4.150961518287659\n",
      "epoch : 5, loss: 2.9164575338363647\n",
      "epoch : 6, loss: 2.226456493139267\n",
      "epoch : 7, loss: 1.7538153678178787\n",
      "epoch : 8, loss: 1.553993508219719\n",
      "epoch : 9, loss: 1.4254101812839508\n",
      "epoch : 10, loss: 1.329359844326973\n",
      "epoch : 11, loss: 1.2307719588279724\n",
      "epoch : 12, loss: 1.1799951940774918\n",
      "epoch : 13, loss: 1.1371780931949615\n",
      "epoch : 14, loss: 1.089996099472046\n",
      "epoch : 15, loss: 1.0457943081855774\n",
      "epoch : 16, loss: 1.0176042169332504\n",
      "epoch : 17, loss: 1.000445768237114\n",
      "epoch : 18, loss: 0.9825780838727951\n",
      "epoch : 19, loss: 0.9758484959602356\n",
      "epoch : 20, loss: 0.9644200801849365\n",
      "epoch : 21, loss: 0.9612149149179459\n",
      "epoch : 22, loss: 0.959821030497551\n",
      "epoch : 23, loss: 0.9624015986919403\n",
      "epoch : 24, loss: 0.9546905905008316\n",
      "epoch : 25, loss: 0.9510115683078766\n",
      "epoch : 26, loss: 0.9441613703966141\n",
      "epoch : 27, loss: 0.9444252997636795\n",
      "epoch : 28, loss: 0.937652200460434\n",
      "epoch : 29, loss: 0.940976470708847\n",
      "epoch : 30, loss: 0.9389292895793915\n",
      "epoch : 31, loss: 0.9291906207799911\n",
      "epoch : 32, loss: 0.9333021342754364\n",
      "epoch : 33, loss: 0.9292399138212204\n",
      "epoch : 34, loss: 0.9262730032205582\n",
      "epoch : 35, loss: 0.9236687421798706\n",
      "epoch : 36, loss: 0.9237734526395798\n",
      "epoch : 37, loss: 0.9238181859254837\n",
      "epoch : 38, loss: 0.9225573241710663\n",
      "epoch : 39, loss: 0.9153963476419449\n",
      "epoch : 40, loss: 0.9122920483350754\n",
      "epoch : 41, loss: 0.9126279950141907\n",
      "epoch : 42, loss: 0.9077179729938507\n",
      "epoch : 43, loss: 0.9070726633071899\n",
      "epoch : 44, loss: 0.9049151986837387\n",
      "epoch : 45, loss: 0.9040922224521637\n",
      "epoch : 46, loss: 0.902387261390686\n",
      "epoch : 47, loss: 0.9026001542806625\n",
      "epoch : 48, loss: 0.8987616449594498\n",
      "epoch : 49, loss: 0.8967295065522194\n",
      "epoch : 50, loss: 0.8957021981477737\n",
      "epoch : 51, loss: 0.8920010402798653\n",
      "epoch : 52, loss: 0.8881451487541199\n",
      "epoch : 53, loss: 0.8860102295875549\n",
      "epoch : 54, loss: 0.8832912892103195\n",
      "epoch : 55, loss: 0.8799677789211273\n",
      "epoch : 56, loss: 0.877637967467308\n",
      "epoch : 57, loss: 0.8736186027526855\n",
      "epoch : 58, loss: 0.8704539313912392\n",
      "epoch : 59, loss: 0.8731558173894882\n",
      "epoch : 60, loss: 0.8765029609203339\n",
      "epoch : 61, loss: 0.8702981472015381\n",
      "epoch : 62, loss: 0.8610685989260674\n",
      "epoch : 63, loss: 0.8581883907318115\n",
      "epoch : 64, loss: 0.8577041923999786\n",
      "epoch : 65, loss: 0.8552004098892212\n",
      "epoch : 66, loss: 0.8549479097127914\n",
      "epoch : 67, loss: 0.8482487499713898\n",
      "epoch : 68, loss: 0.8443263992667198\n",
      "epoch : 69, loss: 0.8490413501858711\n",
      "epoch : 70, loss: 0.8457680270075798\n",
      "epoch : 71, loss: 0.8372049629688263\n",
      "epoch : 72, loss: 0.8414707705378532\n",
      "epoch : 73, loss: 0.834548182785511\n",
      "epoch : 74, loss: 0.8350004851818085\n",
      "epoch : 75, loss: 0.8352929502725601\n",
      "epoch : 76, loss: 0.8372729048132896\n",
      "epoch : 77, loss: 0.8365938439965248\n",
      "epoch : 78, loss: 0.8348719626665115\n",
      "epoch : 79, loss: 0.8360837623476982\n",
      "epoch : 80, loss: 0.8284876719117165\n",
      "epoch : 81, loss: 0.8266698569059372\n",
      "epoch : 82, loss: 0.821210004389286\n",
      "epoch : 83, loss: 0.8260360434651375\n",
      "epoch : 84, loss: 0.8230955749750137\n",
      "epoch : 85, loss: 0.819757804274559\n",
      "epoch : 86, loss: 0.82040785998106\n",
      "epoch : 87, loss: 0.81717399507761\n",
      "epoch : 88, loss: 0.8178510442376137\n",
      "epoch : 89, loss: 0.8170087859034538\n",
      "epoch : 90, loss: 0.815625749528408\n",
      "epoch : 91, loss: 0.8119617477059364\n",
      "epoch : 92, loss: 0.8140252232551575\n",
      "epoch : 93, loss: 0.8141419067978859\n",
      "epoch : 94, loss: 0.8097180128097534\n",
      "epoch : 95, loss: 0.8105132877826691\n",
      "epoch : 96, loss: 0.8087190240621567\n",
      "epoch : 97, loss: 0.8067866116762161\n",
      "epoch : 98, loss: 0.8065168634057045\n",
      "epoch : 99, loss: 0.8064262121915817\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "encoded_line_normal_list = line_train(line_meta_normal_list)\n",
    "encoded_line_abnormal_list = line_train(line_meta_abnormal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "937362d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/recbole_jh/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 243}\n",
      "{0: 204, 1: 0}\n"
     ]
    }
   ],
   "source": [
    "line_concated = encoded_line_normal_list + encoded_line_abnormal_list \n",
    "line_concated = torch.stack(line_concated).cpu().numpy()\n",
    "\n",
    "# Run k-means clustering\n",
    "line_cluster_labels = kmeans.fit_predict(line_concated)\n",
    "\n",
    "line_normal_cnt={0:0, 1:0} # total should be: 243\n",
    "line_abnormal_cnt={0:0, 1:0} # 204\n",
    "\n",
    "\n",
    "for i, cl_label in enumerate(line_cluster_labels):\n",
    "    if i < len(encoded_line_normal_list):\n",
    "        line_normal_cnt[cl_label]+= 1\n",
    "    else:\n",
    "        line_abnormal_cnt[cl_label] += 1\n",
    "    \n",
    "print(line_normal_cnt)\n",
    "print(line_abnormal_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "786e8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-13.3745, -12.9934, -18.4419, -26.6632, -29.4094, -29.3742, -31.2977,\n",
      "        -32.9443, -36.6441, -34.8051, -36.3590, -36.4045, -38.8022, -40.6189,\n",
      "        -40.6635, -24.5543, -10.7123,  -6.6756, -13.1272, -11.0439, -14.8549,\n",
      "        -16.8325, -17.3081, -20.3440, -22.3437, -22.9538, -22.1248, -22.9636,\n",
      "        -22.4643, -25.7804, -27.2800, -16.7124, -11.2664,  -6.6650,  -7.5051,\n",
      "        -12.0915, -17.6410, -21.4354, -24.2447, -27.8242, -29.9402, -31.9817,\n",
      "        -30.9346, -31.9373, -33.5120, -34.3105, -36.6013, -24.6131, -11.9517,\n",
      "        -11.2176, -10.1818, -13.0367, -20.0484, -22.6991, -23.8574, -25.6085,\n",
      "        -27.6730, -28.2072, -29.1583, -31.1095, -32.4300, -32.6938, -33.9177,\n",
      "        -24.2028], device='cuda:0')\n",
      "tensor([ 2.5997, 12.7299, 16.0664, 26.2039, 36.6281, 31.4814, 36.4267, 39.0514,\n",
      "        39.2179, 38.6829, 42.0047, 46.4250, 49.2073, 48.0779, 47.7326, 29.6187,\n",
      "         4.1852,  5.0790,  7.4126,  8.4095, 19.1328, 24.2020, 26.5203, 29.3995,\n",
      "        31.7641, 32.7316, 32.8760, 32.8734, 30.1136, 32.0611, 34.8653, 28.5366,\n",
      "         4.8222,  4.3623,  6.8609,  9.4991, 21.7010, 24.4400, 33.6720, 38.4140,\n",
      "        41.6595, 43.7386, 44.6080, 44.9718, 45.3974, 44.7722, 46.7170, 33.6002,\n",
      "        14.9604, 17.6594, 20.2742, 17.1924, 30.8887, 34.5616, 38.8676, 47.3134,\n",
      "        55.2175, 56.9814, 57.0387, 58.6660, 59.8341, 59.5387, 60.6116, 46.2157],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "idx = 43\n",
    "print(encoded_line_normal_list[idx])\n",
    "print(encoded_line_abnormal_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b7b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cb086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71570a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaaeba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267e73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d75ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7bdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
